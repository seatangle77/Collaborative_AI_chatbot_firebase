<template>
  <div v-if="isRecognizing" style="padding: 8px 16px; color: #409eff">
    ğŸ¤ æ­£åœ¨è¯†åˆ«è¯­éŸ³...
  </div>
  <div class="input-container">
    <!-- âœ… ç”¨æˆ·é€‰æ‹© -->
    <el-select
      v-model="selectedUser"
      placeholder="é€‰æ‹©ç”¨æˆ·"
      class="user-select"
      size="large"
    >
      <el-option
        v-for="(user, userId) in users"
        :key="userId"
        :label="user.name"
        :value="userId"
      />
    </el-select>

    <!-- âœ… æ¶ˆæ¯è¾“å…¥æ¡† -->
    <el-input
      v-model="message"
      placeholder="è¯·è¾“å…¥æ¶ˆæ¯..."
      @input="updateSpeakingDuration"
      @keyup.enter="handleSend"
      class="message-input"
      size="large"
    />

    <!-- âœ… Speaking Duration è¾“å…¥æ¡† (ms) -->
    <el-input
      v-model="speakingDuration"
      type="number"
      placeholder="æ—¶é•¿(ms)"
      class="duration-input"
      size="small"
    />

    <!-- âœ… å‘é€æŒ‰é’® -->
    <el-button type="primary" @click="handleSend" size="large" class="send-btn">
      Send
    </el-button>

    <el-button
      type="success"
      @click="startAudioCapture"
      size="large"
      class="send-btn"
    >
      ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ«
    </el-button>

    <el-button
      type="warning"
      @click="stopAudioCapture"
      size="large"
      class="send-btn"
      :disabled="!isRecognizing"
    >
      ğŸ›‘ ç»“æŸè¯­éŸ³è¯†åˆ«
    </el-button>
  </div>
</template>

<script setup>
import { ref, watch, onMounted } from "vue";
import { recognizeSpeechFromMicrophone } from "../services/azureSpeech";

const props = defineProps({
  users: {
    type: Object,
    default: () => ({}),
  },
  groupId: String,
  messages: {
    type: Array,
    default: () => [],
  },
  isTtsPlaying: {
    type: Boolean,
    default: false,
  },
});

const message = ref("");
const selectedUser = ref(null);
const speakingDuration = ref(null); // âœ… è®©å‰ç«¯æ§åˆ¶ speaking_duration (ms)
const isRecognizing = ref(false);
const autoLoop = ref(true); // æ§åˆ¶æ˜¯å¦è‡ªåŠ¨è¯†åˆ«ä¸‹ä¸€è½®

onMounted(() => {
  const userIds = Object.keys(props.users);
  if (userIds.length > 0 && !selectedUser.value) {
    selectedUser.value = userIds[0];
  }
});

const startAudioCapture = async () => {
  if (props.isTtsPlaying) {
    return;
  }

  isRecognizing.value = true;
  const startTime = performance.now();

  try {
    const resultText = await recognizeSpeechFromMicrophone();
    const endTime = performance.now();
    const duration = Math.round(endTime - startTime);
    speakingDuration.value = duration;

    if (resultText) {
      console.log("ğŸ“ Azure è¯†åˆ«ç»“æœï¼š", resultText);
      message.value = resultText;
      console.log("ğŸ“ å®é™…è¯­éŸ³æ—¶é•¿(ms)ï¼š", duration);
      handleSend(); // âœ… è‡ªåŠ¨å‘é€åç”± handleSend å†³å®šæ˜¯å¦ç»§ç»­è¯†åˆ«
    } else {
      console.warn("âš ï¸ Azure è¿”å›äº†ç©ºå­—ç¬¦ä¸²");
    }
  } catch (err) {
    console.error("âŒ Azure è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼š", err);
  } finally {
    isRecognizing.value = false;
  }
};

const stopAudioCapture = () => {
  autoLoop.value = false;
  isRecognizing.value = false;
};

watch(
  () => props.messages,
  async (newMessages) => {
    const latestMsg = newMessages[newMessages.length - 1];
    const msgId = latestMsg.msgid || latestMsg.msgId; // Use msgId
  }
);

// âœ… **åŠ¨æ€è®¡ç®— speaking_durationï¼ˆä»¥ ms è®¡ç®—ï¼‰**
const updateSpeakingDuration = () => {
  if (message.value.trim()) {
    speakingDuration.value = message.value.length * 50; // å‡è®¾ 1 å­—ç¬¦ â‰ˆ 50ms
  } else {
    speakingDuration.value = null;
  }
};

watch(
  () => props.users,
  (newUsers) => {
    const userIds = Object.keys(newUsers);
    if (userIds.length > 0 && !selectedUser.value) {
      selectedUser.value = userIds[0];
    }
  },
  { immediate: true }
);

const emit = defineEmits(["send-message", "stop-audio-capture"]);

const handleSend = () => {
  if (message.value.trim() && selectedUser.value) {
    emit("send-message", {
      group_id: props.groupId,
      user_id: selectedUser.value,
      message: message.value,
      speaking_duration: speakingDuration.value || null, // âœ… ç¡®ä¿ä¼ å…¥æ¯«ç§’å€¼
    });
    const msgKey = message.value.trim();
    message.value = "";
    speakingDuration.value = null;

    // å‘é€å®Œæˆåç»§ç»­è¯†åˆ«
    if (autoLoop.value) {
      setTimeout(() => startAudioCapture(), 500);
    }
  }
};

defineExpose({
  stopAudioCapture,
});
</script>

<style scoped>
/* âœ… è¾“å…¥æ¡†å®¹å™¨ */
.input-container {
  display: flex;
  align-items: center;
  padding: 12px 16px;
  background: #fff;
  border-top: 1px solid #e0e0e0;
  box-shadow: 0 -2px 6px rgba(0, 0, 0, 0.05);
  border-radius: 0 0 12px 12px;
}

/* âœ… ç”¨æˆ·é€‰æ‹©æ¡† */
.user-select {
  width: 140px;
  margin-right: 12px;
}

/* âœ… æ¶ˆæ¯è¾“å…¥æ¡† */
.message-input {
  flex: 1;
  border-radius: 8px;
  transition: all 0.3s ease-in-out;
}

.message-input:focus-within {
  box-shadow: 0 0 6px rgba(64, 158, 255, 0.6);
}

/* âœ… æ—¶é•¿è¾“å…¥æ¡† */
.duration-input {
  width: 100px;
  margin-left: 8px;
  text-align: center;
  height: 40px;
}

/* âœ… å‘é€æŒ‰é’® */
.send-btn {
  padding: 10px 20px;
  font-size: 16px;
  font-weight: bold;
  border-radius: 8px;
  background: linear-gradient(to right, #409eff, #187bcd);
  transition: all 0.3s ease-in-out;
  margin-left: 12px;
}

.send-btn:hover {
  background: linear-gradient(to right, #5aafff, #1b89e5);
}
</style>
