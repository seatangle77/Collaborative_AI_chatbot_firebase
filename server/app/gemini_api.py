import os
import json
import requests
from dotenv import load_dotenv
from app.database import db
import google.generativeai as genai
import traceback


# âœ… åŠ è½½ .env
load_dotenv()

# âœ… è®¾ç½®ç¯å¢ƒå˜é‡ä¾› SDK ä½¿ç”¨
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
client = genai


def generate_ai_response(bot_id: str, main_prompt: str, history_prompt: str = None,
                         prompt_type: str = "real_time_summary", model: str = "default", agent_id: str = None):
    """
    ä½¿ç”¨ google-genai å®˜æ–¹ Client æ¥å£è°ƒç”¨ Gemini æ¨¡å‹ï¼ˆæ ‡å‡†æ¨èç”¨æ³•ï¼‰
    """
    try:
        # âœ… ä»æ–°ç‰ˆè¡¨è·å– prompt æ•°æ®
        if prompt_type in ["real_time_summary", "cognitive_guidance", "summary_to_knowledge"]:
            if not bot_id:
                raise ValueError(f"{prompt_type} ç±»å‹å¿…é¡»æä¾› bot_id")
            table = "ai_prompt_versions"
            id_field = "ai_bot_id"
            id_value = bot_id
        elif prompt_type in ["term_explanation", "knowledge_followup"]:
            if not agent_id:
                raise ValueError(f"{prompt_type} ç±»å‹å¿…é¡»æä¾› agent_id")
            table = "agent_prompt_versions"
            id_field = "agent_id"
            id_value = agent_id
        else:
            raise ValueError(f"âŒ Unsupported prompt_type: {prompt_type}")

        docs = (
            db.collection(table)
            .where(id_field, "==", id_value)
            .where("prompt_type", "==", prompt_type)
            .where("is_active", "==", True)
            .order_by("created_at", direction=firestore.Query.DESCENDING)
            .limit(1)
            .stream()
        )

        docs = list(docs)
        if not docs:
            raise ValueError(f"âŒ No active prompt found for {prompt_type}")

        system_prompt = docs[0].to_dict()["rendered_prompt"]
        max_words = 150 if prompt_type == "real_time_summary" else 100
        system_prompt = system_prompt.replace("{max_words}", str(max_words))

        # âœ… æ„é€ ç”¨æˆ·å†…å®¹
        if prompt_type == "real_time_summary":
            user_prompt = f"è¯·åœ¨ {max_words} è¯ä»¥å†…æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼š\n\n{main_prompt}"
        elif prompt_type == "cognitive_guidance":
            user_prompt = f"è¯·æ ¹æ®ä»¥ä¸‹è®¨è®ºå†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦å¼•å¯¼å›¢é˜Ÿè¿›ä¸€æ­¥è®¨è®ºï¼Œå¹¶æä¾›çŸ¥è¯†æ”¯æŒï¼š\n\n{main_prompt}"
        elif prompt_type == "term_explanation":
            user_prompt = f"è¯·åœ¨ {max_words} è¯ä»¥å†…è§£é‡Šè¿™ä¸ªæœ¯è¯­ï¼š\n\n{main_prompt}"
        else:
            return f"âŒ ä¸æ”¯æŒçš„ prompt_type: {prompt_type}"

        # âœ… ç»„åˆå†…å®¹ï¼ˆsystem + user + optional historyï¼‰
        contents = [system_prompt, user_prompt]
        if history_prompt:
            contents.append(history_prompt)

        # âœ… æ˜ å°„æ¨¡å‹å
        model = "gemini-2.5-pro-exp-03-25"
        model_name = model

        print("ğŸ“¤ ä½¿ç”¨æ¨¡å‹:", model_name)
        print("ğŸ“¤ ç»„åˆ prompt å†…å®¹å¦‚ä¸‹ï¼š\n", "\n\n".join(contents))

        # âœ… è°ƒç”¨ Gemini APIï¼ˆéæµå¼ç‰ˆæœ¬ï¼‰
        response = client.models.generate_content(
            model=model_name,
            contents=[{"role": "user", "parts": [{"text": "\n\n".join(contents)}]}],
        )

        print("ğŸ“¥ åŸå§‹å“åº”å¯¹è±¡:", response)
        print("ğŸ“¥ å“åº”æ–‡æœ¬å†…å®¹:", response.candidates[0].content.parts[0].text.strip())

        return response.candidates[0].content.parts[0].text.strip()

    except Exception as e:
        print(f"âŒ Gemini API è°ƒç”¨å¤±è´¥: {e}")
        traceback.print_exc()
        return f"AI ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚é”™è¯¯è¯¦æƒ…: {str(e)}"
