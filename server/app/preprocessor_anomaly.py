from app.database import db
import os, json
from datetime import datetime
from datetime import timezone, timedelta
from google.cloud.firestore_v1.base_query import FieldFilter
import time

def parse_iso_time(iso_str):
    if not iso_str:
        return None
    if isinstance(iso_str, dict) and "_seconds" in iso_str:
        # Firestore timestamp dict
        return datetime.fromtimestamp(
            iso_str["_seconds"] + iso_str.get("_nanoseconds", 0) / 1e9,
            tz=timezone.utc
        )
    if iso_str.endswith("Z"):
        iso_str = iso_str.replace("Z", "+00:00")
    try:
        dt = datetime.fromisoformat(iso_str)
        # å…³é”®å¤„ç†ï¼šæ— æ—¶åŒºæ ‡è¯†æ—¶è®¾ä¸ºä¸œå…«åŒº
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone(timedelta(hours=8)))
        return dt
    except Exception:
        return None

def extract_chunk_data_anomaly(round_index: int, start_time: str, end_time: str, group_id: str, member_list: list, current_user: dict) -> dict:
    """
    è·å–å½“å‰ chunk å†…æ‰€æœ‰ç”¨æˆ·çš„è¡Œä¸ºæ•°æ®ï¼Œå‡†å¤‡é€å…¥ GPT å¤„ç†ã€‚
    è¿”å›ç»“æ„åŒ…å«æ¯ä½ç”¨æˆ·çš„è¡Œä¸º/æ ‡ç­¾æ•°æ®ã€‚
    """
    total_start_time = time.time()
    print(f"ğŸ” [æ•°æ®é¢„å¤„ç†] å¼€å§‹æå–group_id={group_id}çš„æ•°æ®ï¼Œæ—¶é—´èŒƒå›´ï¼š{start_time} ~ {end_time}")
    
    # ç»Ÿä¸€è§£æä¸ºdatetimeå¯¹è±¡
    start_time_dt = parse_iso_time(start_time)
    end_time_dt = parse_iso_time(end_time)

    user_ids = [m["user_id"] for m in member_list]
    print(f"ğŸ“‹ [æ•°æ®é¢„å¤„ç†] æ´»è·ƒç”¨æˆ·æ•°é‡ï¼š{len(user_ids)}")

    def filter_time_range(item_start, item_end=None):
        start = parse_iso_time(item_start)
        end = parse_iso_time(item_end) if item_end else None
        if not start:
            return False
        if end:
            # æœ‰å¼€å§‹å’Œç»“æŸï¼Œåˆ¤æ–­åŒºé—´æ˜¯å¦æœ‰äº¤é›†
            return (start_time_dt <= start <= end_time_dt) or (start_time_dt <= end <= end_time_dt)
        else:
            return start_time_dt <= start <= end_time_dt

    # æŸ¥è¯¢1: speech_transcripts
    query1_start = time.time()
    speech_transcripts = [doc.to_dict() for doc in db.collection("speech_transcripts").where(filter=FieldFilter("group_id", "==", group_id)).stream()]
    speech_transcripts = [s for s in speech_transcripts if filter_time_range(s.get("start"), s.get("end"))]
    # æŒ‰ user_id ç»Ÿè®¡å‘è¨€æ¬¡æ•°
    from collections import Counter
    speech_counts = Counter(s["user_id"] for s in speech_transcripts if s.get("user_id"))
    for m in member_list:
        m["speech_count"] = speech_counts.get(m["user_id"], 0)
    query1_duration = time.time() - query1_start
    print(f"ğŸ¤ [æ•°æ®é¢„å¤„ç†] æŸ¥è¯¢1-speech_transcriptså®Œæˆï¼Œè€—æ—¶{query1_duration:.2f}ç§’ï¼Œæ‰¾åˆ°{len(speech_transcripts)}æ¡è®°å½•")

    # æŸ¥è¯¢2: note_edit_history
    query2_start = time.time()
    note_edit_history = []
    if member_list:
        for m in member_list:
            uid = m["user_id"]
            query = db.collection("note_edit_history").where(filter=FieldFilter("userId", "==", uid)).stream()
            note_edit_history.extend([doc.to_dict() for doc in query])

    filtered_note_edit_history = []
    for e in note_edit_history:
        ts = parse_iso_time(e.get("updatedAt"))
        if ts and start_time_dt <= ts <= end_time_dt:
            filtered_note_edit_history.append(e)
    note_edit_history = filtered_note_edit_history
    query2_duration = time.time() - query2_start
    print(f"ğŸ“ [æ•°æ®é¢„å¤„ç†] æŸ¥è¯¢2-note_edit_historyå®Œæˆï¼Œè€—æ—¶{query2_duration:.2f}ç§’ï¼Œæ‰¾åˆ°{len(note_edit_history)}æ¡è®°å½•")

    # æŸ¥è¯¢3: pageBehaviorLogs
    query3_start = time.time()
    pageBehaviorLogs = []
    all_logs = db.collection("pageBehaviorLogs").stream()
    count_all = 0
    count_group_matched = 0
    for doc in all_logs:
        count_all += 1
        data = doc.to_dict()
        user_info = data.get("behaviorData", {}).get("user", {})
        if user_info.get("group_id") == group_id:
            count_group_matched += 1
            pageBehaviorLogs.append(data)
    filtered_logs = []
    for b in pageBehaviorLogs:
        ws = parse_iso_time(b.get("windowStart"))
        we = parse_iso_time(b.get("windowEnd"))
        if ws and we and start_time_dt <= ws <= end_time_dt:
            filtered_logs.append(b)
    pageBehaviorLogs = filtered_logs
    query3_duration = time.time() - query3_start
    print(f"ğŸ–¥ï¸ [æ•°æ®é¢„å¤„ç†] æŸ¥è¯¢3-pageBehaviorLogså®Œæˆï¼Œè€—æ—¶{query3_duration:.2f}ç§’ï¼Œæ€»è®°å½•{count_all}ï¼ŒåŒ¹é…ç»„{count_group_matched}ï¼Œæ—¶é—´è¿‡æ»¤å{len(pageBehaviorLogs)}æ¡")

    # æŸ¥è¯¢4: note_contents
    query4_start = time.time()
    note_contents_all = []
    if member_list:
        uid = member_list[0]["user_id"]
        query = db.collection("note_contents")\
            .where(filter=FieldFilter("userId", "==", uid))\
            .stream()
        note_contents_all.extend([doc.to_dict() for doc in query])

    note_contents = []
    for n in note_contents_all:
        ts = parse_iso_time(n.get("updatedAt", ""))
        if ts and start_time_dt <= ts <= end_time_dt:
            note_contents.append(n)
    query4_duration = time.time() - query4_start
    print(f"ğŸ“„ [æ•°æ®é¢„å¤„ç†] æŸ¥è¯¢4-note_contentså®Œæˆï¼Œè€—æ—¶{query4_duration:.2f}ç§’ï¼Œæ‰¾åˆ°{len(note_contents)}æ¡è®°å½•")

    chunk_data = {
        "time_range": {
            "start": start_time,
            "end": end_time
        },
        "group_id": group_id,
        "users": member_list,
        "raw_tables": {
            "speech_transcripts": speech_transcripts,
            "note_edit_history": note_edit_history,
            "pageBehaviorLogs": pageBehaviorLogs,
            "unique_note_contents": note_contents
        },
        "speech_counts": speech_counts,
        "current_user": current_user
    }

    # æŸ¥è¯¢5: anomaly_analysis_resultså†å²
    query5_start = time.time()
    try:
        results = db.collection("anomaly_analysis_results") \
            .where("group_id", "==", group_id) \
            .where("current_user.user_id", "==", current_user["user_id"]) \
            .order_by("created_at", direction="DESCENDING") \
            .limit(2) \
            .stream()
        history = [doc.to_dict() for doc in results]
        anomaly_history = []
        for h in history:
            anomaly_history.append({
                "detail": h.get("detail"),
                "glasses_summary": h.get("glasses_summary"),
                "summary": h.get("summary"),
                "user_data_summary": h.get("user_data_summary"),
                "start_time": h.get("start_time"),
                "end_time": h.get("end_time")
            })
        if len(anomaly_history) == 0:
            chunk_data["anomaly_history"] = None
        else:
            chunk_data["anomaly_history"] = anomaly_history
        query5_duration = time.time() - query5_start
        print(f"ğŸ“š [æ•°æ®é¢„å¤„ç†] æŸ¥è¯¢5-anomaly_analysis_resultså†å²å®Œæˆï¼Œè€—æ—¶{query5_duration:.2f}ç§’ï¼Œæ‰¾åˆ°{len(anomaly_history)}æ¡å†å²è®°å½•")
    except Exception as e:
        query5_duration = time.time() - query5_start
        print(f"âŒ [æ•°æ®é¢„å¤„ç†] æŸ¥è¯¢5-anomaly_analysis_resultså†å²å¤±è´¥ï¼Œè€—æ—¶{query5_duration:.2f}ç§’ï¼š{e}")
        chunk_data["anomaly_history"] = None

    # ä¿å­˜è°ƒè¯•æ–‡ä»¶
    debug_start = time.time()
    from uuid import uuid4
    os.makedirs("debug_anomaly_outputs", exist_ok=True)
    debug_file_path = f"debug_anomaly_outputs/chunk_data_{uuid4().hex}.json"
    with open(debug_file_path, "w", encoding="utf-8") as f:
        json.dump(chunk_data, f, ensure_ascii=False, indent=2)
    debug_duration = time.time() - debug_start
    print(f"ğŸ’¾ [æ•°æ®é¢„å¤„ç†] ä¿å­˜è°ƒè¯•æ–‡ä»¶å®Œæˆï¼Œè€—æ—¶{debug_duration:.2f}ç§’")

    total_duration = time.time() - total_start_time
    print(f"âœ… [æ•°æ®é¢„å¤„ç†] group_id={group_id}æ•°æ®æå–å®Œæˆï¼Œæ€»è€—æ—¶{total_duration:.2f}ç§’")

    return chunk_data


def build_cognitive_anomaly_input(chunk_data: dict) -> dict:
    """
    æ„å»ºç”¨äºè®¤çŸ¥åˆ†æä»»åŠ¡çš„ GPT è¾“å…¥ç»“æ„ã€‚
    """
    return {
        "group_id": chunk_data["group_id"],
        "time_range": chunk_data["time_range"],
        "users": chunk_data["users"],
        "speech_transcripts": chunk_data["raw_tables"]["speech_transcripts"],
        "unique_note_contents": chunk_data["raw_tables"]["unique_note_contents"]
    }


def build_behavior_anomaly_input(chunk_data: dict) -> dict:
    """
    æ„å»ºç”¨äºè¡Œä¸ºåˆ†æä»»åŠ¡çš„ GPT è¾“å…¥ç»“æ„ã€‚
    """
    return {
        "group_id": chunk_data["group_id"],
        "time_range": chunk_data["time_range"],
        "users": chunk_data["users"],
        "note_edit_history": chunk_data["raw_tables"]["note_edit_history"],
        "pageBehaviorLogs": chunk_data["raw_tables"]["pageBehaviorLogs"],
        "speech_counts": chunk_data.get("speech_counts", {})
    }


def build_attention_anomaly_input(chunk_data: dict) -> dict:
    """
    æ„å»ºç”¨äºæ³¨æ„åŠ›åˆ†æä»»åŠ¡çš„ GPT è¾“å…¥ç»“æ„ã€‚
    """
    return {
        "group_id": chunk_data["group_id"],
        "time_range": chunk_data["time_range"],
        "users": chunk_data["users"],
        "note_edit_history": chunk_data["raw_tables"]["note_edit_history"],
        "pageBehaviorLogs": chunk_data["raw_tables"]["pageBehaviorLogs"],
        "speech_transcripts": chunk_data["raw_tables"]["speech_transcripts"],
    }

def build_anomaly_history_input(chunk_data: dict) -> dict:
    """
    æ„å»ºç”¨äºå¼‚å¸¸å†å²åˆ†æçš„è¾“å…¥ç»“æ„ã€‚
    """
    return {
        "current_user": chunk_data.get("current_user"),
        "anomaly_history": chunk_data.get("anomaly_history")
    }