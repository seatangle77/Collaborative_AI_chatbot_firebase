import json
import os
import time
import traceback
import uuid
from datetime import timezone, datetime
from typing import Union

import google.generativeai as genai
from dotenv import load_dotenv
from pydantic import BaseModel

from server.app.anomaly_preprocessor import (
    get_group_members_simple, parse_iso_time, extract_chunk_data_anomaly
)
from server.app.database import db
from server.app.logger.logger_loader import logger


class Member(BaseModel):
    id: str
    name: str

class CurrentUser(BaseModel):
    user_id: str
    name: str
    device_token: str

# ä¼˜å…ˆåŠ è½½ .env.localï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå†åŠ è½½ .env
load_dotenv('.env.local')
load_dotenv()

# âœ… è®¾ç½®ç¯å¢ƒå˜é‡ä¾› SDK ä½¿ç”¨
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

def ai_analyze_all_anomalies(chunk_data_with_local_analyze: dict) -> tuple[str, dict]:
    total_start_time = time.time()
    logger.info(f"ğŸš€ [AIåˆ†æ] å¼€å§‹è°ƒç”¨Gemini AIè¿›è¡Œå¼‚å¸¸åˆ†æ...")

    model = genai.GenerativeModel("gemini-1.5-flash")

    prompt_text = f"""
ã€ç”¨æˆ·è¡Œä¸ºæ•°æ®ã€‘='''{chunk_data_with_local_analyze}'''

ã€ä»»åŠ¡æè¿°ã€‘='''
ä½ æ˜¯ä¸€ä¸ªå¤šç»´åº¦å°ç»„åä½œåˆ†æä¸“å®¶ã€‚ç³»ç»Ÿå·²ç»ä¸ºä¸€ä¸ªå°ç»„ä¸­çš„ä¸‰ä½æˆå‘˜æä¾›äº†å®Œæ•´çš„è¡Œä¸ºè¯„åˆ†ç»“æœï¼ŒåŒ…æ‹¬å‘è¨€ã€ç¼–è¾‘ã€æµè§ˆç­‰çº§å’Œæ€»å¾—åˆ†ï¼ˆæ— éœ€ä½ é‡æ–°åˆ¤æ–­ï¼‰ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š

ğŸ”¹ åŸºäº total_level åˆ¤æ–­è¯¥æˆå‘˜çš„å‚ä¸çŠ¶æ€ï¼›
ğŸ”¹ è¾“å‡ºæ¸©å’Œé¼“åŠ±çš„çœ¼é•œæç¤ºè¯­ï¼ˆglasses_summaryï¼‰ï¼›
ğŸ”¹ ç»™å‡ºè¯¥æˆå‘˜çš„çŠ¶æ€ç±»å‹ã€è¡Œä¸ºç»“æ„æè¿°ã€å»ºè®®ä¸è¯æ®ï¼›
ğŸ”¹ æä¾›æ›´è¯¦ç»†çš„å¤šè§’åº¦åˆ†æï¼ŒåŒ…æ‹¬å†å²å¯¹æ¯”ã€å°ç»„å¯¹æ¯”ä¸åä½œå»ºè®®ï¼›
ğŸ”¹ è¾“å‡ºè¯¥å°ç»„çš„æ•´ä½“å‚ä¸ç»“æ„ group_distributionï¼ŒåŒ…æ‹¬äº”ç±»å‚ä¸ç­‰çº§çš„æ•°é‡ç»Ÿè®¡ä¸ç»“æ„æ´å¯Ÿï¼ˆå¦‚ç±»å‹ã€é£é™©ã€å»ºè®®ï¼‰ï¼›
ğŸ”¹ æ˜ç¡®æŒ‡å‡ºæ˜¯å¦éœ€è¦è¿›è¡Œæç¤ºï¼ˆå­—æ®µ should_notifyï¼‰ã€‚

ğŸ¯ æ‰€æœ‰æç¤ºå¿…é¡»è¯­æ°”æ­£å‘ã€äº²å’Œï¼Œé¿å…æ‰¹è¯„ï¼›
ğŸ¯ glasses_summary å¿…é¡»é€‚é…æ™ºèƒ½çœ¼é•œçš„å°å±å¹•ï¼Œå†…å®¹ç®€æ´ã€å‹å¥½ï¼Œ**å¿…é¡»ä½¿ç”¨é¢œæ–‡å­—ï¼ˆå¦‚ ^_^ã€>_<ã€(à¹‘â€¢Ì€ã…‚â€¢Ì)Ùˆâœ§ï¼‰ï¼Œä¸¥ç¦ä½¿ç”¨ emoji å›¾æ ‡ï¼ˆå¦‚ ğŸ˜Šã€ğŸ‘ ç­‰ï¼‰**ï¼›
ğŸ¯ è¾“å‡ºå†…å®¹å¿…é¡»ä¸¥æ ¼ç¬¦åˆä¸‹æ–¹æ ¼å¼ã€‚
'''

ã€å‚ä¸ç­‰çº§ä¸æç¤ºè§„åˆ™ã€‘='''
- total_level = "No Participation" â†’ æ˜ç¡®æç¤ºæ¿€æ´»ï¼›
- total_level = "Low Participation" â†’ é¼“åŠ±è¡¨è¾¾ä¸æ“ä½œï¼›
- total_level = "Normal Participation" â†’ æ— éœ€æç¤ºï¼›
- total_level = "High Participation" â†’ æ¸©å’Œæç¤ºåä½œå¹³è¡¡ï¼›
- total_level = "Dominant" â†’ å§”å©‰æç¤ºç•™å‡ºä»–äººç©ºé—´ã€‚
'''

ã€è¾“å‡ºç»“æ„ã€‘='''
è¯·ä¸ºæ¯ä½æˆå‘˜è¾“å‡ºä»¥ä¸‹ JSON ç»“æ„ï¼ˆå…± 3 ç»„ï¼‰ï¼š

{{
  "ç”¨æˆ·ID": {{
    "user_name": "ç”¨æˆ·å",
    "summary": "ä¸€å¥è¯æ€»ç»“å½“å‰çŠ¶æ€",
    "glasses_summary": "å»ºè®®[æ¸©å’Œæç¤º]+é¢œæ–‡å­—è¡¨æƒ…,8ä¸ªå­—ä»¥å†…",
    "should_notify": true æˆ– false,
    "detail": {{
      "type": "å‚ä¸çŠ¶æ€ç±»å‹ï¼Œå¦‚ Low Participation",
      "status": "ç®€æ´æè¿°è¯¥æˆå‘˜çš„å½“å‰è¡Œä¸ºç»“æ„",
      "evidence": "- å‘è¨€ç­‰çº§ï¼šHigh Speechï¼ˆæ—¶é•¿ï¼š76.04sï¼Œå æ¯”ï¼š63.37%ï¼‰\\n- ç¼–è¾‘ç­‰çº§ï¼šNormal Editï¼ˆæ¬¡æ•°ï¼š2ï¼Œå­—ç¬¦æ•°ï¼š637ï¼‰\\n- æµè§ˆç­‰çº§ï¼šNormal Browsingï¼ˆé¡µé¢æ•°ï¼š2ï¼Œæµè§ˆæ—¶é•¿ï¼š61.13sï¼Œå æ¯”ï¼š50.94%ï¼‰",
      "suggestion": "è¡Œä¸ºå±‚é¢çš„æ”¹å–„å»ºè®®ï¼Œå¦‚ä¸»åŠ¨è¡¨è¾¾ã€ååŒå‚ä¸ç­‰"
    }},
    "more_info": {{
      "detailed_reason": "ä½ ä¸ºä½•åˆ¤æ–­è¯¥æˆå‘˜ä¸ºè¯¥çŠ¶æ€çš„è¯¦ç»†è§£é‡Šï¼Œå¼•ç”¨å…·ä½“æ•°æ®è¿›è¡Œæ¨ç†ï¼Œå¦‚å‘è¨€æ—¶é•¿ã€ç¼–è¾‘å­—æ•°ã€mouseè¡Œä¸ºç­‰",
      "history_comparison": "ä¸è¯¥æˆå‘˜è¿‡å¾€è½®æ¬¡çš„æ¯”è¾ƒåˆ†æï¼Œå¦‚â€œæœ¬è½®å‘è¨€æ—¶é•¿æ¯”ä¸Šä¸€è½®å¢åŠ  20sâ€",
      "group_comparison": "ä¸å½“å‰ç»„å†…å…¶ä»–æˆå‘˜çš„å¯¹æ¯”è¯´æ˜ï¼Œå¦‚â€œå‘è¨€æœ€å¤šï¼Œæ¯”å¹³å‡å€¼é«˜å‡º 40%â€",
      "collaboration_suggestion": "ç»“åˆååŒè§’åº¦çš„å…·ä½“å»ºè®®ï¼Œä¾‹å¦‚ä¸»åŠ¨è®©å‡ºç©ºé—´ã€é‚€è¯·ä»–äººè¡¨è¾¾ç­‰",
      "extra_data": "å¯è¡¥å…… mouse_action_countã€mouse_durationã€mouse_percentã€total_score ç­‰ä¿¡æ¯"
    }},
    "group_distribution": {{
      "no": X,
      "low": X,
      "normal": X,
      "high": X,
      "dominant": X,
      "group_type": "ç»“æ„ç±»å‹ï¼Œå¦‚ å¤±è¡¡å‹ / å‡è¡¡å‹ / é«˜å‚ä¸ç»„ / ä½å‚ä¸ç»„",
      "group_risk": "ç»“æ„æ½œåœ¨é£é™©ï¼Œå¦‚ ä¸»å¯¼è€…æ˜æ˜¾ + å¤šäººä½å‚ä¸",
      "action_hint": "ç¾¤ä½“å±‚çº§å»ºè®®ï¼Œå¦‚ æ¿€åŠ±ä½å‚ä¸æˆå‘˜ + é¼“åŠ±ä¸»å¯¼è€…ç•™å‡ºç©ºé—´"
    }}
  }},
  ...
}}
'''

ã€æ³¨æ„äº‹é¡¹ã€‘='''
- æ‰€æœ‰å­—æ®µå¿…é¡»å¡«å†™å®Œæ•´ï¼Œä¸å¾—ç•™ç©ºæˆ–ä½¿ç”¨æ¨¡æ¿å ä½ç¬¦ï¼›
- evidence å¿…é¡»åŒ…å«è¡Œä¸ºç­‰çº§ + åŸå§‹æ•°æ®ï¼ˆå¦‚å‘è¨€æ—¶é•¿ã€ç¼–è¾‘æ¬¡æ•°ã€é¡µé¢æ•°ã€æµè§ˆæ—¶é•¿ç­‰ï¼‰ï¼›
- more_info å­—æ®µéœ€ç»“åˆè¡Œä¸ºæ•°æ®è¿›è¡Œæ·±å…¥åˆ†æä¸æ¯”è¾ƒï¼Œä¸å¾—ä½¿ç”¨ç¬¼ç»Ÿè¯­è¨€ï¼›
- group_distribution ç»“æ„ä¸­å¿…é¡»åŒ…å« group_typeã€group_riskã€action_hint ä¸‰é¡¹å†…å®¹ï¼›
- è‹¥æˆå‘˜æ— éœ€æç¤ºï¼Œglasses_summary ä»åº”è¾“å‡ºç©ºå­—ç¬¦ä¸²ï¼Œshould_notify åº”ä¸º falseï¼›
'''

ä½ ç°åœ¨æ”¶åˆ° 3 ä½æˆå‘˜çš„æ•°æ®ï¼Œè¯·è¾“å‡ºä¸Šè¿°ç»“æ„ã€‚
"""

    logger.info("ğŸš€ [AIåˆ†æ] å¼€å§‹è°ƒç”¨ [Anomaly AI ç»¼åˆåˆ†æ] ...")
    response = model.generate_content(
        contents=[{"role": "user", "parts": [{"text": prompt_text}]}],
        generation_config=genai.types.GenerationConfig(temperature=0.7)
    )
    logger.info(f"âœ… [AIåˆ†æ] [Anomaly AI] è¿”å›ç»“æœï¼š", response.text)

    total_duration = time.time() - total_start_time
    logger.info(f"âœ… [AIåˆ†æ] Gemini AIå¼‚å¸¸åˆ†æå®Œæˆï¼Œæ€»è€—æ—¶{total_duration:.2f}ç§’")

    return prompt_text, {"raw_response": response.text}

def ai_analyze_anomaly_status(group_id: str, chunk_data: dict):
    total_start_time = time.time()
    logger.info(f"ğŸš€ [å¼‚å¸¸åˆ†æ] å¼€å§‹åˆ†ægroup_id={group_id}...")

    start_time = chunk_data['time_range']['start']
    end_time = chunk_data['time_range']['end']

    # é˜¶æ®µ2: AIåˆ†æ
    prompt, ai_analyze_result = ai_analyze_all_anomalies(chunk_data)

    # é˜¶æ®µ3: ç»“æœè§£æ
    try:
        if isinstance(ai_analyze_result.get("raw_response"), str):
            markdown_json = ai_analyze_result["raw_response"]
            # å»é™¤Markdownæ ‡è®°
            json_str = markdown_json.strip('```json').strip('\n').strip('```').strip()
            ai_analyze_result_json = json.loads(json_str)

            try:
                # é˜¶æ®µ4: æ–‡ä»¶å­˜å‚¨
                stage4_start = time.time()
                os.makedirs("analysis_outputs", exist_ok=True)
                file_name = f"analysis_outputs/ai_analysis_{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}.json"
                with open(file_name, "w", encoding="utf-8") as f:
                    json.dump(ai_analyze_result_json, f, ensure_ascii=False, indent=2)
                stage4_duration = time.time() - stage4_start
                logger.info(f"ğŸ’¾ [å¼‚å¸¸åˆ†æ] é˜¶æ®µ4-æ–‡ä»¶å­˜å‚¨å®Œæˆï¼Œè€—æ—¶{stage4_duration:.2f}ç§’")

                # é˜¶æ®µ5: æ•°æ®åº“å­˜å‚¨
                stage5_start = time.time()
                # æ–°å»º anomaly_analysis_files è¡¨å¹¶æ’å…¥å†…å®¹
                file_id = str(uuid.uuid4())
                db.collection("anomaly_raw_json_in_out").document(file_id).set({
                    "id": file_id,
                    "group_id": group_id,
                    "created_at": end_time,
                    "input": prompt,
                    "output": ai_analyze_result_json
                })

                # æ–°å»ºanomaly_analysis_group_resultsè¡¨å¹¶æ’å…¥æ•°æ®
                analysis_id = str(uuid.uuid4())
                db.collection("anomaly_analysis_group_results").document(analysis_id).set({
                    "id": analysis_id,
                    "group_id": group_id,
                    "start_time": start_time,
                    "end_time": end_time,
                    "raw_response": ai_analyze_result_json,
                    "created_at": end_time
                })
                stage5_duration = time.time() - stage5_start
                logger.info(f"ğŸ—„ï¸ [å¼‚å¸¸åˆ†æ] é˜¶æ®µ5-æ•°æ®åº“å­˜å‚¨å®Œæˆï¼Œè€—æ—¶{stage5_duration:.2f}ç§’")

                total_duration = time.time() - total_start_time
                logger.info(f"âœ… [å¼‚å¸¸åˆ†æ] group_id={group_id}åˆ†æå®Œæˆï¼Œæ€»è€—æ—¶{total_duration:.2f}ç§’")
            except Exception as e:
                logger.error('AIè§£æç»“æœä¿å­˜å¼‚å¸¸ï¼š %s' % traceback.format_exc())

                # å¼‚å¸¸æƒ…å†µä¿å­˜
                # é˜¶æ®µ4: æ–‡ä»¶å­˜å‚¨
                logger.warning(f"ğŸ’¾ [å¼‚å¸¸åˆ†æ] ç»“æœå¼‚å¸¸ï¼Œä¿å­˜ç°åœº")
                os.makedirs("analysis_outputs", exist_ok=True)
                file_name = f"analysis_outputs/ai_analysis_{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}.json"
                with open(file_name, "w", encoding="utf-8") as f:
                    json.dump(ai_analyze_result, f, ensure_ascii=False, indent=2)

                # é˜¶æ®µ5: æ•°æ®åº“å­˜å‚¨
                stage5_start = time.time()
                file_id = str(uuid.uuid4())
                db.collection("anomaly_raw_json_in_out").document(file_id).set({
                    "id": file_id,
                    "group_id": group_id,
                    "created_at": datetime.now(timezone.utc).isoformat(),
                    "input": prompt,
                    "output": ai_analyze_result
                })
                stage5_duration = time.time() - stage5_start
                logger.info(f"ğŸ—„ï¸ [å¼‚å¸¸åˆ†æ] ç»“æœå¼‚å¸¸ï¼Œä¿å­˜ç°åœºã€‚æ•°æ®åº“å­˜å‚¨å®Œæˆï¼Œè€—æ—¶{stage5_duration:.2f}ç§’")

                total_duration = time.time() - total_start_time
                logger.info(f"âœ… [å¼‚å¸¸åˆ†æ] ç»“æœå¼‚å¸¸ï¼Œä¿å­˜ç°åœºã€‚group_id={group_id}åˆ†æå®Œæˆï¼Œæ€»è€—æ—¶{total_duration:.2f}ç§’")

            # è¿”å›ç»™å‰ç«¯æ›´å¤šä¿¡æ¯
            return ai_analyze_result_json
    except Exception as e:
        logger.error('è§£æAIå“åº”å¤±è´¥ï¼š %s' % traceback.format_exc())

    return {}

def calculate_total_score_and_level(speech_score_dict, edit_score_dict, browser_score_dict, chunk_data):
    user_ids = [user['user_id'] for user in chunk_data.get('users', [])]
    total_score = {}
    total_level = {}
    for uid in user_ids:
        speech = speech_score_dict.get(uid, 0)
        edit = edit_score_dict.get(uid, 0)
        browser = browser_score_dict.get(uid, 0)
        score = speech * 0.7 + edit * 0.15 + browser * 0.15
        total_score[uid] = round(score, 3)
        # ç­‰çº§åˆ¤å®š
        if score == 0:
            level = "No Participation"
        elif 0 < score <= 0.3:
            level = "Low Participation"
        elif 0.3 < score < 0.7:
            level = "Normal Participation"
        elif 0.7 <= score < 0.9:
            level = "High Participation"
        else:  # 0.9 <= score <= 1
            level = "Dominant"
        total_level[uid] = level
    return total_score, total_level


def classify_speech_level(speech_map, total_speech, total_seconds, chunk_data):
    user_ids = [user['user_id'] for user in chunk_data.get('users', [])]
    speech_durations = [speech_map.get(uid, 0) for uid in user_ids]
    speech_percents = [d / total_speech if total_speech else 0 for d in speech_durations]

    # åˆ†ç±»
    speech_level = {}
    speech_score = {}
    for idx, uid in enumerate(user_ids):
        duration = speech_durations[idx]
        percent = speech_percents[idx]
        if duration == 0:
            speech_level[uid] = "No Speech"
            speech_score[uid] = 0
        elif (total_speech < total_seconds / 3.0):
            # æ‰€æœ‰äººå‘è¨€æ—¶é•¿ä¸åˆ°æ€»æ—¶é•¿3åˆ†ä¹‹ä¸€
            speech_level[uid] = "Low Speech"
            speech_score[uid] = 0.3
        else:
            # æ‰€æœ‰äººå‘è¨€å¸‚åœºè¶…è¿‡æ€»æ—¶é•¿3åˆ†ä¹‹ä¸€ï¼ŒæŒ‰æ¯ä¸ªäººçš„å‘è¨€ç›¸å¯¹æ¯”ä¾‹è®¡ç®—
            if percent < 0.15:
                speech_level[uid] = "Low Speech"
                speech_score[uid] = 0.3
            elif percent >= 0.6:
                speech_level[uid] = "High Speech"
                speech_score[uid] = 1
            else:
                speech_level[uid] = "Normal Speech"
                speech_score[uid] = 0.7
    return speech_level, speech_score

def classify_note_edit_level(note_edit_stats, total_edit_char_count, chunk_data):
    user_ids = [user['user_id'] for user in chunk_data.get('users', [])]
    edit_level = {}
    edit_score = {}

    for uid in user_ids:
        count = note_edit_stats.get(uid, {}).get('note_edit_count', 0)
        chars = note_edit_stats.get(uid, {}).get('note_edit_char_count', 0)
        percent = chars / total_edit_char_count if total_edit_char_count else 0
        if count == 0:
            edit_level[uid] = "No Edit"
            edit_score[uid] = 0
        elif percent < 0.3:
            edit_level[uid] = "Few Edits"
            edit_score[uid] = 0.3
        elif 0.3 <= percent < 0.7:
            edit_level[uid] = "Normal Edit"
            edit_score[uid] = 0.7
        elif percent >= 0.7:
            edit_level[uid] = "Frequent Edit"
            edit_score[uid] = 1
        else:
            edit_level[uid] = "No Edit"
            edit_score[uid] = 0
    return edit_level, edit_score

def classify_browser_behavior_level(page_stats, chunk_data):
    user_ids = [user['user_id'] for user in chunk_data.get('users', [])]
    browser_level = {}
    browser_score = {}
    for uid in user_ids:
        stats = page_stats.get(uid, {})
        action_count = stats.get('mouse_action_count', 0)
        try:
            percent = float(str(stats.get('mouse_percent', '0')).replace('%', ''))
        except Exception:
            percent = 0
        if action_count == 0:
            browser_level[uid] = "No Browsing"
            browser_score[uid] = 0
        elif percent < 30:
            browser_level[uid] = "Few Browsing"
            browser_score[uid] = 0.3
        elif 30 <= percent < 70:
            browser_level[uid] = "Normal Browsing"
            browser_score[uid] = 0.7
        elif percent >= 70:
            browser_level[uid] = "Frequent Browsing"
            browser_score[uid] = 1
        else:
            browser_level[uid] = "No Browsing"
            browser_score[uid] = 0
    return browser_level, browser_score

def local_analyze_anomaly_status(chunk_data) -> tuple[dict, dict]:
    """
    ç»Ÿè®¡ï¼š
    1. speech_transcriptsï¼šæŒ‰userç»Ÿè®¡æ€»è¯´è¯æ—¶é•¿ã€å time_rangeç™¾åˆ†æ¯”ã€‚
    2. pageBehaviorLogsï¼šæŒ‰userç»Ÿè®¡æµè§ˆç½‘é¡µæ•°ã€æ€»actionæ¬¡æ•°ã€é¼ æ ‡æ“ä½œæ€»æ—¶é•¿åŠå time_rangeç™¾åˆ†æ¯”ã€‚
    è¿”å›ï¼š{user_id: {...}}
    """
    time_start_dt = parse_iso_time(chunk_data['time_range']['start'])
    time_end_dt = parse_iso_time(chunk_data['time_range']['end'])
    total_seconds = (time_end_dt - time_start_dt).total_seconds() if time_start_dt and time_end_dt else 1


    # speech_transcriptsç»Ÿè®¡
    # è¯­è¨€è¾“å‡ºå¯èƒ½è·¨è¶Šç»Ÿè®¡å‘¨æœŸï¼Œåªè®¡ç®—è½åœ¨å½“å‰è¯­è¨€å‘¨æœŸçš„æ—¶é•¿
    speech_map = {}
    for item in chunk_data.get('raw_tables', {}).get('speech_transcripts', []):
        uid = item.get('user_id')
        start = parse_iso_time(item.get('start'))
        end = parse_iso_time(item.get('end'))
        if start < time_start_dt:
            start = time_start_dt
        if end > time_end_dt:
            end = time_end_dt
        if uid:
            speech_map.setdefault(uid, 0)
            speech_map[uid] += (end - start).total_seconds()
    total_speech = sum(speech_map.values())

    # pageBehaviorLogsç»Ÿè®¡
    page_logs = chunk_data.get('raw_tables', {}).get('pageBehaviorLogs', {})
    page_stats = {}
    for uname, pdata in page_logs.items():
        user = pdata.get('user', {})
        uid = user.get('user_id')
        tabHistory = pdata.get('tabHistory', [])
        page_count = len(tabHistory)
        action_count = 0
        mousemove_duration = 0.0
        for tab in tabHistory:
            for log in tab.get('tabBehaviorLogs', []):
                action_count += log.get('action_count', 0)
                if log.get('type') in ['mousemove', 'scroll']:
                    st = parse_iso_time(log.get('startTime'))
                    et = parse_iso_time(log.get('endTime'))
                    if st and et:
                        mousemove_duration += (et - st).total_seconds()
        mousemove_percent = round(mousemove_duration / total_seconds * 100, 2) if total_seconds else 0
        page_stats[uid] = {
            'page_count': page_count,
            'mouse_action_count': action_count,
            'mouse_duration': f"{round(mousemove_duration, 2)}s",
            'mouse_percent': f"{mousemove_percent}%"
        }
    # note_edit_history ç»Ÿè®¡
    note_edit_logs = chunk_data.get('raw_tables', {}).get('note_edit_history', [])
    note_edit_stats = {}
    total_edit_char_count = 0
    for item in note_edit_logs:
        uid = item.get('userId')
        if uid not in note_edit_stats:
            note_edit_stats[uid] = {
                'note_edit_count': 0,
                'note_edit_char_count': 0,
            }
        note_edit_stats[uid]['note_edit_count'] += 1
        for edit in item.get('delta', []):
            if "insert" in edit:
                note_edit_stats[uid]['note_edit_char_count'] += len(edit.get('insert', ""))
                total_edit_char_count += len(edit.get('insert', ""))
            elif "delete" in edit:
                note_edit_stats[uid]['note_edit_char_count'] += edit.get('delete', 0)
                total_edit_char_count += edit.get('delete', 0)


    # ç»Ÿè®¡å‘è¨€ç­‰çº§å’Œåˆ†æ•°
    speech_level_dict, speech_score_dict = classify_speech_level(speech_map, total_speech, total_seconds, chunk_data)
    # ç»Ÿè®¡ç¼–è¾‘ç­‰çº§å’Œåˆ†æ•°
    edit_level_dict, edit_score_dict = classify_note_edit_level(note_edit_stats, total_edit_char_count, chunk_data)
    # ç»Ÿè®¡æµè§ˆå™¨è¡Œä¸ºç­‰çº§å’Œåˆ†æ•°
    browser_level_dict, browser_score_dict = classify_browser_behavior_level(page_stats, chunk_data)

    # è®¡ç®—æ€»åˆ†å’Œæ€»ç­‰çº§
    total_score_dict, total_level_dict = calculate_total_score_and_level(speech_score_dict, edit_score_dict, browser_score_dict, chunk_data)

    # åˆå¹¶ç»“æœ
    local_analyze_result = {}
    for user in chunk_data.get('users', []):
        uid = user['user_id']
        uname = user.get('name', '')
        speech_duration = round(speech_map.get(uid, 0), 2)
        speech_percent = round(speech_duration / total_seconds * 100, 2) if total_seconds else 0
        page_info = page_stats.get(uid, {'page_count':0,'mouse_action_count':0,'mouse_duration':'0s','mouse_percent':'0%'})
        local_analyze_result[uid] = {
            'name': uname,
            'speech_duration': f"{speech_duration}s",
            'speech_percent': f"{speech_percent}%",
            'page_count': page_info.get('page_count', 0),
            'mouse_action_count': page_info.get('mouse_action_count', 0),
            'mouse_duration': page_info.get('mouse_duration', '0s'),
            'mouse_percent': page_info.get('mouse_percent', '0%'),
            'note_edit_count': note_edit_stats.get(uid, {}).get('note_edit_count', 0),
            'note_edit_char_count': note_edit_stats.get(uid, {}).get('note_edit_char_count', 0),
            'speech_level': speech_level_dict.get(uid, "No Speech"),
            'speech_level_score': speech_score_dict.get(uid, 0),
            'note_edit_level': edit_level_dict.get(uid, "No Edit"),
            'note_edit_score': edit_score_dict.get(uid, 0),
            'browser_level': browser_level_dict.get(uid, "No Browsing"),
            'browser_score': browser_score_dict.get(uid, 0),
            'total_score': total_score_dict.get(uid, 0),
            'total_level': total_level_dict.get(uid, "No Participation")
        }

    # åˆå¹¶æœ¬åœ°åˆ†æç»“æœåˆ° chunk_data
    chunk_data['local_analysis_result'] = local_analyze_result

    return chunk_data, local_analyze_result

def local_analyze(group_id:str, start_time:Union[datetime,str], end_time:Union[datetime,str], is_save_debug_info:bool = True) -> tuple[dict, dict]:
    # é˜¶æ®µ1: è·å–æˆå‘˜ä¿¡æ¯
    start_time_1 = time.time()
    members = get_group_members_simple(group_id)
    logger.info(f"ğŸ“Š [å¼‚å¸¸åˆ†æ] é˜¶æ®µ1-è·å–æˆå‘˜ä¿¡æ¯å®Œæˆï¼Œè€—æ—¶{(time.time() - start_time_1):.2f}ç§’")

    # é˜¶æ®µ2: æ•°æ®é¢„å¤„ç†
    start_time_2 = time.time()
    if isinstance(start_time, datetime):
        start_time_str = start_time.strftime("%Y-%m-%dT%H:%M:%S")
    else:
        start_time_str = start_time
    if isinstance(end_time, datetime):
        end_time_str = end_time.strftime("%Y-%m-%dT%H:%M:%S")
    else:
        end_time_str = end_time
    raw_data, increment = extract_chunk_data_anomaly(
        group_id=group_id,
        round_index=1,
        start_time=start_time_str,
        end_time=end_time_str,
        member_list=members
    )
    logger.info(f"ğŸ“Š [å¼‚å¸¸åˆ†æ] é˜¶æ®µ2-æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶{time.time() - start_time_2:.2f}ç§’")
    if increment <= 0:
        logger.warning("[å¼‚å¸¸åˆ†æ] ç”¨æˆ·æ´»åŠ¨æ•°æ®å¢é‡ä¸º0ï¼Œä¸åšåˆ†æ")
        return {}, {}

    # é˜¶æ®µ3ï¼šæœ¬åœ°æ•°æ®åˆ†æ
    chunk_data_with_local_analyze, local_analyze_result = local_analyze_anomaly_status(raw_data)

    # é˜¶æ®µ4ï¼šä¿å­˜è°ƒè¯•æ–‡ä»¶
    if is_save_debug_info:
        os.makedirs("analysis_outputs", exist_ok=True)
        debug_file_path = f"analysis_outputs/local_analysis_{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}.json"
        with open(debug_file_path, "w", encoding="utf-8") as f:
            json.dump(chunk_data_with_local_analyze, f, ensure_ascii=False, indent=2)

        # é˜¶æ®µ5: æ•°æ®åº“å­˜å‚¨
        stage5_start = time.time()
        file_id = str(uuid.uuid4())
        db.collection("anomaly_local_analyze").document(file_id).set({
            "id": file_id,
            "group_id": group_id,
            "created_at": datetime.now(timezone.utc).isoformat(),
            "output": chunk_data_with_local_analyze
        })
        stage5_duration = time.time() - stage5_start
        logger.info(f"ğŸ—„ï¸ [å¼‚å¸¸åˆ†æ] æœ¬åœ°åˆ†æã€‚æ•°æ®åº“å­˜å‚¨å®Œæˆï¼Œè€—æ—¶{stage5_duration:.2f}ç§’")

    return chunk_data_with_local_analyze, local_analyze_result




if __name__ == '__main__':
    ...
    # # åˆ†ææ–‡ä»¶è®°å½•
    # input_file = "../debug_anomaly_outputs/chunk_data_18b4c9cf636e45e8829738b96f4f53bb_merge1.json"
    # with open(input_file, 'r', encoding='utf-8') as f:
    #     logs = json.load(f)
    # print(json.dumps(local_analyze_anomaly_status(logs), ensure_ascii=False, indent=2))

    # æŸ¥è¯¢æ•°æ®ï¼Œåˆ†æç»“æœ
    group_id = "cc8f1d29-7a49-4975-95dc-7ac94aefc04b"
    start_time_str = "2025-07-10T07:02:27"
    end_time_str = "2025-07-10T07:04:27"
    members = get_group_members_simple(group_id)
    raw_data, increment = extract_chunk_data_anomaly(
        group_id=group_id,
        round_index=1,
        start_time=start_time_str,
        end_time=end_time_str,
        member_list=members
    )
    chunk_data_with_local_result, local_analyze_result = local_analyze_anomaly_status(raw_data)
    # prompt, ai_analyze_result = ai_analyze_all_anomalies(chunk_data_with_local_result)
    # if isinstance(ai_analyze_result.get("raw_response"), str):
    #     markdown_json = ai_analyze_result["raw_response"]
    #     # å»é™¤Markdownæ ‡è®°
    #     json_str = markdown_json.strip('```json').strip('\n').strip('```').strip()
    #     print(json.dumps(json.loads(json_str), ensure_ascii=False, indent=2))

    ai_analyze_anomaly_status(group_id, chunk_data_with_local_result)
