import json
import os
import re
import time
import uuid
from datetime import timezone, datetime

import google.generativeai as genai
import numpy as np
from dotenv import load_dotenv
from pydantic import BaseModel

from server.app.anomaly_preprocessor import (
    build_anomaly_history_input
)
from server.app.database import db
from server.app.logger.logger_loader import logger


class Member(BaseModel):
    id: str
    name: str

class CurrentUser(BaseModel):
    user_id: str
    name: str
    device_token: str

# ä¼˜å…ˆåŠ è½½ .env.localï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå†åŠ è½½ .env
load_dotenv('.env.local')
load_dotenv()

# âœ… è®¾ç½®ç¯å¢ƒå˜é‡ä¾› SDK ä½¿ç”¨
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

def ai_analyze_all_anomalies(chunk_data: dict) -> dict:
    total_start_time = time.time()
    logger.info(f"ğŸš€ [AIåˆ†æ] å¼€å§‹è°ƒç”¨Gemini AIè¿›è¡Œå¼‚å¸¸åˆ†æ...")

    model = genai.GenerativeModel("gemini-1.5-flash")

    # é˜¶æ®µ1: æ„å»ºè¾“å…¥æ•°æ®
    stage1_start = time.time()
    anomaly_history_input = None
    anomaly_history_json = None
    try:
        anomaly_history_input = build_anomaly_history_input(chunk_data)
        anomaly_history_json = json.dumps(anomaly_history_input, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error("æœªèƒ½è·å–anomaly_history_inputï¼š", e)
        anomaly_history_json = "null"

    current_user_json = json.dumps(chunk_data.get('current_user', {}), ensure_ascii=False, indent=2)
    speech_counts_json = json.dumps(chunk_data.get('speech_counts', {}), ensure_ascii=False, indent=2)
    speech_durations_json = json.dumps(chunk_data.get('speech_durations', {}), ensure_ascii=False, indent=2)
    stage1_duration = time.time() - stage1_start
    logger.info(f"ğŸ“‹ [AIåˆ†æ] é˜¶æ®µ1-æ„å»ºè¾“å…¥æ•°æ®å®Œæˆï¼Œè€—æ—¶{stage1_duration:.2f}ç§’")

    # é˜¶æ®µ2: æ„å»ºæç¤ºè¯
    stage2_start = time.time()
    prompt_text = f"""
ä½ æ˜¯ä¸€ä¸ªå¤šç»´åº¦å°ç»„åä½œåˆ†æä¸“å®¶ã€‚ç³»ç»Ÿå·²ç»ä¸ºä¸€ä¸ªå°ç»„ä¸­çš„ä¸‰ä½æˆå‘˜æä¾›äº†å®Œæ•´çš„è¡Œä¸ºè¯„åˆ†ç»“æœï¼ŒåŒ…æ‹¬å‘è¨€ã€ç¼–è¾‘ã€æµè§ˆç­‰çº§å’Œæ€»å¾—åˆ†ï¼ˆä¸éœ€è¦ä½ é‡æ–°åˆ¤æ–­ï¼‰ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š

ğŸ”¹ åŸºäº total_level åˆ¤æ–­è¯¥æˆå‘˜çš„å‚ä¸çŠ¶æ€ï¼›
ğŸ”¹ è¾“å‡ºæ¸©å’Œé¼“åŠ±çš„çœ¼é•œæç¤ºè¯­ï¼ˆglasses_summaryï¼‰ï¼›
ğŸ”¹ ç»™å‡ºè¯¥æˆå‘˜çš„çŠ¶æ€ç±»å‹ã€è¡Œä¸ºç»“æ„æè¿°ã€å»ºè®®ä¸è¯æ®ï¼›
ğŸ”¹ æä¾›æ›´è¯¦ç»†çš„å¤šè§’åº¦åˆ†æï¼ŒåŒ…æ‹¬å†å²å¯¹æ¯”ã€å°ç»„å¯¹æ¯”ä¸åä½œå»ºè®®ï¼›
ğŸ”¹ è¡¥å……è¯¥å°ç»„åœ¨å„ä¸ªæ€»å‚ä¸ç­‰çº§ï¼ˆlow/normal/high/dominantï¼‰ä¸­çš„æˆå‘˜äººæ•°ç»Ÿè®¡ï¼ˆgroup_distribution å­—æ®µï¼‰ã€‚

ğŸ¯ æ‰€æœ‰æç¤ºå¿…é¡»è¯­æ°”æ­£å‘ã€äº²å’Œï¼Œä¸å¾—æ‰¹è¯„ï¼›
ğŸ¯ glasses_summary åº”é€‚é…çœ¼é•œå°å±å¹•ï¼Œä½¿ç”¨ä¸€å¥ç®€æ´ä¸­æ–‡ï¼Œå¯å«é¢œæ–‡å­—ï¼›
ğŸ¯ è¾“å‡ºå­—æ®µç»“æ„å¿…é¡»å®Œå…¨ç¬¦åˆä¸‹æ–¹æ ¼å¼ï¼›

ã€å‚ä¸ç­‰çº§ä¸æç¤ºè§„åˆ™ã€‘
- total_level = "No Participation" â†’ æ˜ç¡®æç¤ºæ¿€æ´»ï¼›
- total_level = "Low Participation" â†’ é¼“åŠ±è¡¨è¾¾ä¸æ“ä½œï¼›
- total_level = "Normal Participation" â†’ æ— éœ€æç¤ºï¼›
- total_level = "High Participation" â†’ æ¸©å’Œæç¤ºåä½œå¹³è¡¡ï¼›
- total_level = "Dominant" â†’ å§”å©‰æç¤ºç•™å‡ºä»–äººç©ºé—´ã€‚

ã€è¾“å‡ºç»“æ„ã€‘
è¯·ä¸ºæ¯ä½æˆå‘˜è¾“å‡ºä»¥ä¸‹ JSON ç»“æ„ï¼ˆå…± 3 ç»„ï¼‰ï¼š

{{
  "æˆå‘˜å": {{
    "summary": "ä¸€å¥è¯æ€»ç»“å½“å‰çŠ¶æ€",
    "glasses_summary": "ä½ å½“å‰[çŠ¶æ€]ï¼Œå»ºè®®[æ¸©å’Œæç¤º]",
    "detail": {{
      "type": "å‚ä¸çŠ¶æ€ç±»å‹ï¼Œå¦‚ Low Participation",
      "status": "ç®€æ´æè¿°è¯¥æˆå‘˜çš„å½“å‰è¡Œä¸ºç»“æ„",
      "evidence": "- å‘è¨€ç­‰çº§ï¼š{speech_level}\\n- ç¼–è¾‘ç­‰çº§ï¼š{note_edit_level}\\n- æµè§ˆç­‰çº§ï¼š{browser_level}",
      "suggestion": "è¡Œä¸ºå±‚é¢çš„æ”¹å–„å»ºè®®ï¼Œå¦‚ä¸»åŠ¨è¡¨è¾¾ã€ååŒå‚ä¸ç­‰"
    }},
    "more_info": {{
      "detailed_reason": "ä½ ä¸ºä½•åˆ¤æ–­è¯¥æˆå‘˜ä¸ºè¯¥çŠ¶æ€çš„è¯¦ç»†è§£é‡Š",
      "history_comparison": "ä¸è¯¥æˆå‘˜è¿‡å¾€è½®æ¬¡çš„æ¯”è¾ƒåˆ†æ",
      "group_comparison": "ä¸å½“å‰ç»„å†…å…¶ä»–æˆå‘˜çš„å¯¹æ¯”è¯´æ˜",
      "collaboration_suggestion": "ç»“åˆååŒè§’åº¦çš„å…·ä½“å»ºè®®ï¼Œä¾‹å¦‚å¸¦åŠ¨ä»–äºº/è®©å‡ºè¡¨è¾¾ç©ºé—´ç­‰"
    }},
    "group_distribution": {{
      "no": X,
      "low": X,
      "normal": X,
      "high": X,
      "dominant": X
    }}
  }},
  ...
}}

ğŸ“Œ æ³¨æ„äº‹é¡¹ï¼š
- æ‰€æœ‰å­—æ®µå¿…é¡»å¡«å†™å®Œæ•´ï¼Œé¿å…è¾“å‡ºæ¨¡æ¿å ä½ç¬¦ï¼›
- è‹¥æˆå‘˜æ— éœ€æé†’ï¼Œ`glasses_summary` ä»éœ€è¾“å‡ºç©ºå­—ç¬¦ä¸²ï¼Œä½† `should_notify` å­—æ®µåº”ä¸º falseï¼›
- group_distribution ç»Ÿè®¡çš„æ˜¯è¯¥å°ç»„ä¸­ä¸åŒ total_level çš„äººæ•°ï¼ˆä½ å°†æ”¶åˆ°æˆ–æ¨ç†ï¼‰ï¼›

ä½ ç°åœ¨å°†æ”¶åˆ° 3 ä½æˆå‘˜çš„æ•°æ®ï¼Œè¯·è¾“å‡ºä¸Šè¿°ç»“æ„ã€‚
"""
    stage2_duration = time.time() - stage2_start
    logger.info(f"ğŸ“ [AIåˆ†æ] é˜¶æ®µ2-æ„å»ºæç¤ºè¯å®Œæˆï¼Œè€—æ—¶{stage2_duration:.2f}ç§’")

    # é˜¶æ®µ3: è°ƒç”¨AIæ¨¡å‹
    stage3_start = time.time()
    logger.info("ğŸš€ [AIåˆ†æ] å¼€å§‹è°ƒç”¨ [Anomaly AI ç»¼åˆåˆ†æ] ...")
    response = model.generate_content(
        contents=[{"role": "user", "parts": [{"text": prompt_text}]}],
        generation_config=genai.types.GenerationConfig(temperature=0.7)
    )
    stage3_duration = time.time() - stage3_start
    logger.info(f"âœ… [AIåˆ†æ] é˜¶æ®µ3-AIè°ƒç”¨å®Œæˆï¼Œè€—æ—¶{stage3_duration:.2f}ç§’")
    logger.info(f"âœ… [AIåˆ†æ] [Anomaly AI] è¿”å›ç»“æœï¼š", response.text)

    total_duration = time.time() - total_start_time
    logger.info(f"âœ… [AIåˆ†æ] Gemini AIå¼‚å¸¸åˆ†æå®Œæˆï¼Œæ€»è€—æ—¶{total_duration:.2f}ç§’")

    return {"raw_response": response.text}

async def ai_analyze_anomaly_status(group_id: str, start_time: str, end_time: str, chunk_data: dict):
    total_start_time = time.time()
    logger.info(f"ğŸš€ [å¼‚å¸¸åˆ†æ] å¼€å§‹åˆ†ægroup_id={group_id}...")

    # é˜¶æ®µ2: AIåˆ†æ
    stage2_start = time.time()
    result = ai_analyze_all_anomalies(chunk_data)
    stage2_duration = time.time() - stage2_start
    logger.info(f"ğŸ¤– [å¼‚å¸¸åˆ†æ] é˜¶æ®µ2-AIåˆ†æå®Œæˆï¼Œè€—æ—¶{stage2_duration:.2f}ç§’")

    # é˜¶æ®µ3: ç»“æœè§£æ
    stage3_start = time.time()
    # è§£æAIè¿”å›çš„JSONç»“æœ

    summary = None
    glasses_summary = None
    detail = None
    user_data_summary = None
    more_info = None
    score = None
    should_push = False
    try:
        if isinstance(result.get("raw_response"), str):
            raw = result["raw_response"]
            # ç”¨æ­£åˆ™æå–å‡º {...} éƒ¨åˆ†
            match = re.search(r"{[\s\S]*}", raw)
            if match:
                json_str = match.group(0)
                parsed_result = json.loads(json_str)
                summary = parsed_result.get("summary")
                glasses_summary = parsed_result.get("glasses_summary", "ä½ å½“å‰çŠ¶æ€éœ€è¦å…³æ³¨")
                detail = parsed_result.get("detail")
                user_data_summary = parsed_result.get("user_data_summary")
                more_info = parsed_result.get("more_info")
                score = parsed_result.get("score")

                # æ ¹æ®scoreçš„çŠ¶æ€è¯„åˆ†å’Œå†…å®¹ç›¸ä¼¼åº¦è¯„åˆ†åˆ¤æ–­æ˜¯å¦æ¨é€
                if score and isinstance(score, dict):
                    state_score = score.get("state_score")
                    content_similarity_score = score.get("content_similarity_score")
                    should_push = False
                    if state_score is not None and content_similarity_score is not None:
                        should_push = (state_score < 25 or state_score > 75) and (content_similarity_score < 50)
                        logger.info(
                            f"ğŸ“Š [å¼‚å¸¸åˆ†æ] çŠ¶æ€è¯„åˆ†ï¼š{state_score}ï¼Œå†…å®¹ç›¸ä¼¼åº¦è¯„åˆ†ï¼š{content_similarity_score}ï¼Œæ¨é€é˜ˆå€¼ï¼šçŠ¶æ€è¯„åˆ†<25æˆ–>75ï¼Œå†…å®¹ç›¸ä¼¼åº¦è¯„åˆ†<50ï¼Œæ˜¯å¦æ¨é€ï¼š{should_push}")
                    else:
                        should_push = True  # å¦‚æœæ²¡æœ‰è¯„åˆ†ä¿¡æ¯ï¼Œé»˜è®¤æ¨é€
                        logger.info(f"âš ï¸ [å¼‚å¸¸åˆ†æ] æœªæ‰¾åˆ°å®Œæ•´è¯„åˆ†ä¿¡æ¯ï¼Œé»˜è®¤æ¨é€")
                else:
                    should_push = False  # å¦‚æœæ²¡æœ‰scoreä¿¡æ¯ï¼Œé»˜è®¤ä¸æ¨é€
                    logger.info(f"âš ï¸ [å¼‚å¸¸åˆ†æ] æœªæ‰¾åˆ°è¯„åˆ†ä¿¡æ¯ï¼Œé»˜è®¤ä¸æ¨é€")
            else:
                glasses_summary = "ä½ å½“å‰çŠ¶æ€éœ€è¦å…³æ³¨"
                should_push = True
        else:
            glasses_summary = "ä½ å½“å‰çŠ¶æ€éœ€è¦å…³æ³¨"
            should_push = True
    except Exception as e:
        logger.info("è§£æAIå“åº”å¤±è´¥ï¼š", e)
        glasses_summary = "ä½ å½“å‰çŠ¶æ€éœ€è¦å…³æ³¨"
        should_push = True
    stage3_duration = time.time() - stage3_start
    logger.info(f"ğŸ“ [å¼‚å¸¸åˆ†æ] é˜¶æ®µ3-ç»“æœè§£æå®Œæˆï¼Œè€—æ—¶{stage3_duration:.2f}ç§’")

    # é˜¶æ®µ4: æ–‡ä»¶å­˜å‚¨
    stage4_start = time.time()
    # ä¿å­˜åˆ†æç»“æœä¸ºæ–‡ä»¶

    os.makedirs("analysis_outputs", exist_ok=True)
    file_name = f"analysis_outputs/anomaly_{uuid.uuid4()}_{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}.json"
    with open(file_name, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    stage4_duration = time.time() - stage4_start
    logger.info(f"ğŸ’¾ [å¼‚å¸¸åˆ†æ] é˜¶æ®µ4-æ–‡ä»¶å­˜å‚¨å®Œæˆï¼Œè€—æ—¶{stage4_duration:.2f}ç§’")

    # é˜¶æ®µ5: æ•°æ®åº“å­˜å‚¨
    stage5_start = time.time()
    # æ–°å»º anomaly_analysis_files è¡¨å¹¶æ’å…¥å†…å®¹
    file_id = str(uuid.uuid4())
    db.collection("anomaly_raw_json_input").document(file_id).set({
        "id": file_id,
        "group_id": group_id,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "raw_json": result  # å®Œæ•´åˆ†æå†…å®¹
    })

    # æ–°å»ºanomaly_analysis_resultsè¡¨å¹¶æ’å…¥æ•°æ®
    analysis_id = str(uuid.uuid4())
    db.collection("anomaly_analysis_results").document(analysis_id).set({
        "id": analysis_id,
        "group_id": group_id,
        "start_time": start_time,
        "end_time": end_time,
        "raw_response": result.get("raw_response"),
        "summary": summary,
        "glasses_summary": glasses_summary,
        "detail": detail,
        "user_data_summary": user_data_summary,
        "more_info": more_info,
        "score": score,
        "should_push": should_push,
        "created_at": datetime.now(timezone.utc).isoformat()
    })
    stage5_duration = time.time() - stage5_start
    logger.info(f"ğŸ—„ï¸ [å¼‚å¸¸åˆ†æ] é˜¶æ®µ5-æ•°æ®åº“å­˜å‚¨å®Œæˆï¼Œè€—æ—¶{stage5_duration:.2f}ç§’")


    total_duration = time.time() - total_start_time
    logger.info(f"âœ… [å¼‚å¸¸åˆ†æ] group_id={group_id}åˆ†æå®Œæˆï¼Œæ€»è€—æ—¶{total_duration:.2f}ç§’")

    # è¿”å›ç»™å‰ç«¯æ›´å¤šä¿¡æ¯
    return {
        "raw_response": result.get("raw_response"),
        "summary": summary,
        "glasses_summary": glasses_summary,
        "detail": detail,
        "user_data_summary": user_data_summary,
        "more_info": more_info,
        "score": score,
        "should_push": should_push,
        "group_id": group_id,
        "start_time": start_time,
        "end_time": end_time,
        "analysis_id": analysis_id,
        "anomaly_analysis_results_id": analysis_id  # æ·»åŠ å…¼å®¹å­—æ®µ
    }

def calculate_total_score_and_level(speech_score_dict, edit_score_dict, browser_score_dict, chunk_data):
    user_ids = [user['user_id'] for user in chunk_data.get('users', [])]
    total_score = {}
    total_level = {}
    for uid in user_ids:
        speech = speech_score_dict.get(uid, 0)
        edit = edit_score_dict.get(uid, 0)
        browser = browser_score_dict.get(uid, 0)
        score = speech * 0.7 + edit * 0.15 + browser * 0.15
        total_score[uid] = round(score, 3)
        # ç­‰çº§åˆ¤å®š
        if score == 0:
            level = "No Participation"
        elif 0 < score <= 0.3:
            level = "Low Participation"
        elif 0.3 < score < 0.7:
            level = "Normal Participation"
        elif 0.7 <= score < 0.9:
            level = "High Participation"
        else:  # 0.9 <= score <= 1
            level = "Dominant"
        total_level[uid] = level
    return total_score, total_level

def local_analyze_anomaly_status(chunk_data):
    """
    ç»Ÿè®¡ï¼š
    1. speech_transcriptsï¼šæŒ‰userç»Ÿè®¡æ€»è¯´è¯æ—¶é•¿ã€å time_rangeç™¾åˆ†æ¯”ã€‚
    2. pageBehaviorLogsï¼šæŒ‰userç»Ÿè®¡æµè§ˆç½‘é¡µæ•°ã€æ€»actionæ¬¡æ•°ã€é¼ æ ‡æ“ä½œæ€»æ—¶é•¿åŠå time_rangeç™¾åˆ†æ¯”ã€‚
    è¿”å›ï¼š{user_id: {...}}
    """
    def parse_time(t):
        if not t:
            return None
        try:
            return datetime.fromisoformat(t.replace('Z', '+00:00'))
        except Exception:
            return None

    time_start = parse_time(chunk_data['time_range']['start'])
    time_end = parse_time(chunk_data['time_range']['end'])
    total_seconds = (time_end - time_start).total_seconds() if time_start and time_end else 1

    # speech_transcriptsç»Ÿè®¡
    speech_map = {}
    for item in chunk_data.get('raw_tables', {}).get('speech_transcripts', []):
        uid = item.get('user_id')
        dur = item.get('duration', 0)
        if uid:
            speech_map.setdefault(uid, 0)
            speech_map[uid] += dur
    total_speech = sum(speech_map.values())

    # pageBehaviorLogsç»Ÿè®¡
    page_logs = chunk_data.get('raw_tables', {}).get('pageBehaviorLogs', {})
    page_stats = {}
    for uname, pdata in page_logs.items():
        user = pdata.get('user', {})
        uid = user.get('user_id')
        tabHistory = pdata.get('tabHistory', [])
        page_count = len(tabHistory)
        action_count = 0
        mousemove_duration = 0.0
        for tab in tabHistory:
            for log in tab.get('tabBehaviorLogs', []):
                action_count += log.get('action_count', 0)
                if log.get('type') in ['mousemove', 'scroll']:
                    st = parse_time(log.get('startTime'))
                    et = parse_time(log.get('endTime'))
                    if st and et:
                        mousemove_duration += (et - st).total_seconds()
        mousemove_percent = round(mousemove_duration / total_seconds * 100, 2) if total_seconds else 0
        page_stats[uid] = {
            'page_count': page_count,
            'mouse_action_count': action_count,
            'mouse_duration': f"{round(mousemove_duration, 2)}s",
            'mouse_percent': f"{mousemove_percent}%"
        }
    # note_edit_history ç»Ÿè®¡
    note_edit_logs = chunk_data.get('raw_tables', {}).get('note_edit_history', [])
    note_edit_stats = {}
    total_edit_char_count = 0
    for item in note_edit_logs:
        uid = item.get('userId')
        if uid not in note_edit_stats:
            note_edit_stats[uid] = {
                'note_edit_count': 0,
                'note_edit_char_count': 0,
            }
        note_edit_stats[uid]['note_edit_count'] += 1
        for edit in item.get('delta', []):
            if "insert" in edit:
                note_edit_stats[uid]['note_edit_char_count'] += len(edit.get('insert', ""))
                total_edit_char_count += len(edit.get('insert', ""))
            elif "delete" in edit:
                note_edit_stats[uid]['note_edit_char_count'] += edit.get('delete', 0)
                total_edit_char_count += edit.get('delete', 0)


    # ç»Ÿè®¡å‘è¨€ç­‰çº§å’Œåˆ†æ•°
    speech_level_dict, speech_score_dict = classify_speech_level(speech_map, total_speech, total_seconds, chunk_data)
    # ç»Ÿè®¡ç¼–è¾‘ç­‰çº§å’Œåˆ†æ•°
    edit_level_dict, edit_score_dict = classify_note_edit_level(note_edit_stats, total_edit_char_count, chunk_data)
    # ç»Ÿè®¡æµè§ˆå™¨è¡Œä¸ºç­‰çº§å’Œåˆ†æ•°
    browser_level_dict, browser_score_dict = classify_browser_behavior_level(page_stats, chunk_data)

    # è®¡ç®—æ€»åˆ†å’Œæ€»ç­‰çº§
    total_score_dict, total_level_dict = calculate_total_score_and_level(speech_score_dict, edit_score_dict, browser_score_dict, chunk_data)

    # åˆå¹¶ç»“æœ
    result = {}
    for user in chunk_data.get('users', []):
        uid = user['user_id']
        uname = user.get('name', '')
        speech_duration = round(speech_map.get(uid, 0), 2)
        speech_percent = round(speech_duration / total_seconds * 100, 2) if total_seconds else 0
        page_info = page_stats.get(uid, {'page_count':0,'mouse_action_count':0,'mouse_duration':'0s','mouse_percent':'0%'})
        result[uid] = {
            'name': uname,
            'speech_duration': f"{speech_duration}s",
            'speech_percent': f"{speech_percent}%",
            'page_count': page_info.get('page_count', 0),
            'mouse_action_count': page_info.get('mouse_action_count', 0),
            'mouse_duration': page_info.get('mouse_duration', '0s'),
            'mouse_percent': page_info.get('mouse_percent', '0%'),
            'note_edit_count': note_edit_stats.get(uid, {}).get('note_edit_count', 0),
            'note_edit_char_count': note_edit_stats.get(uid, {}).get('note_edit_char_count', 0),
            'speech_level': speech_level_dict.get(uid, "No Speech"),
            'speech_level_score': speech_score_dict.get(uid, 0),
            'note_edit_level': edit_level_dict.get(uid, "No Edit"),
            'note_edit_score': edit_score_dict.get(uid, 0),
            'browser_level': browser_level_dict.get(uid, "No Browsing"),
            'browser_score': browser_score_dict.get(uid, 0),
            'total_score': total_score_dict.get(uid, 0),
            'total_level': total_level_dict.get(uid, "No Participation")
        }
    return result

def classify_speech_level(speech_map, total_speech, total_seconds, chunk_data):
    user_ids = [user['user_id'] for user in chunk_data.get('users', [])]
    speech_durations = [speech_map.get(uid, 0) for uid in user_ids]
    speech_percents = [d / total_speech if total_speech else 0 for d in speech_durations]

    # åˆ†ç±»
    speech_level = {}
    speech_score = {}
    for idx, uid in enumerate(user_ids):
        duration = speech_durations[idx]
        percent = speech_percents[idx]
        if duration == 0:
            speech_level[uid] = "No Speech"
            speech_score[uid] = 0
        elif (total_speech < total_seconds / 3.0):
            # æ‰€æœ‰äººå‘è¨€æ—¶é•¿ä¸åˆ°æ€»æ—¶é•¿3åˆ†ä¹‹ä¸€
            speech_level[uid] = "Low Speech"
            speech_score[uid] = 0.3
        else:
            if percent < 0.3:
                speech_level[uid] = "Low Speech"
                speech_score[uid] = 0.3
            elif percent >= 0.7:
                speech_level[uid] = "High Speech"
                speech_score[uid] = 1
            else:
                speech_level[uid] = "Normal Speech"
                speech_score[uid] = 0.7
    return speech_level, speech_score

def classify_note_edit_level(note_edit_stats, total_edit_char_count, chunk_data):
    user_ids = [user['user_id'] for user in chunk_data.get('users', [])]
    edit_level = {}
    edit_score = {}

    for uid in user_ids:
        count = note_edit_stats.get(uid, {}).get('note_edit_count', 0)
        chars = note_edit_stats.get(uid, {}).get('note_edit_char_count', 0)
        percent = chars / total_edit_char_count if total_edit_char_count else 0
        if count == 0:
            edit_level[uid] = "No Edit"
            edit_score[uid] = 0
        elif percent < 0.3:
            edit_level[uid] = "Few Edits"
            edit_score[uid] = 0.3
        elif 0.3 <= percent < 0.7:
            edit_level[uid] = "Normal Edit"
            edit_score[uid] = 0.7
        elif percent >= 0.7:
            edit_level[uid] = "Frequent Edit"
            edit_score[uid] = 1
        else:
            edit_level[uid] = "No Edit"
            edit_score[uid] = 0
    return edit_level, edit_score

def classify_browser_behavior_level(page_stats, chunk_data):
    user_ids = [user['user_id'] for user in chunk_data.get('users', [])]
    browser_level = {}
    browser_score = {}
    for uid in user_ids:
        stats = page_stats.get(uid, {})
        action_count = stats.get('mouse_action_count', 0)
        try:
            percent = float(str(stats.get('mouse_percent', '0')).replace('%', ''))
        except Exception:
            percent = 0
        if action_count == 0:
            browser_level[uid] = "No Browsing"
            browser_score[uid] = 0
        elif percent < 30:
            browser_level[uid] = "Few Browsing"
            browser_score[uid] = 0.3
        elif 30 <= percent < 70:
            browser_level[uid] = "Normal Browsing"
            browser_score[uid] = 0.7
        elif percent >= 70:
            browser_level[uid] = "Frequent Browsing"
            browser_score[uid] = 1
        else:
            browser_level[uid] = "No Browsing"
            browser_score[uid] = 0
    return browser_level, browser_score

if __name__ == '__main__':
    ...

    input_file = "../debug_anomaly_outputs/chunk_data_18b4c9cf636e45e8829738b96f4f53bb_merge1.json"
    with open(input_file, 'r', encoding='utf-8') as f:
        logs = json.load(f)
    print(json.dumps(local_analyze_anomaly_status(logs), ensure_ascii=False, indent=2))