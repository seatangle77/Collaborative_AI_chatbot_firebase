import asyncio
import queue
import threading
import time
import traceback
from datetime import datetime, timezone

from server.app.anomaly_analyze import ai_analyze_anomaly_status, local_analyze
from server.app.database import db
from server.app.jpush_api import send_jpush_notification
from server.app.logger.logger_loader import logger
from server.app.websocket_routes import push_anomaly_analysis_result

# _group_id = None
_group_id = None
_users_info = None
# 120ç§’çš„é—´éš”
_default_interval_seconds = 120
# ç”¨æˆ·å®šåˆ¶è‡ªå·±çš„æç¤ºé—´éš”æ—¶é—´ï¼Œ(user_id, interval_seconds)
_user_notify_interval_seconds = {}
# ç”¨æˆ·æœ€åä¸€æ¬¡é€šçŸ¥æ—¶é—´
_user_notify_last_time = {}

# æ‰§è¡Œåˆ†æä»»åŠ¡çš„çº¿ç¨‹
_notify_thread = None
_analyze_thread = None
_ai_analyze_thread = None
_stop_analyze = True

_ai_analyze_q = queue.Queue(1000)
_ai_analyze_result_history = []
_local_analyze_result_history = []

# å¯åŠ¨æ—¶é—´å’Œå¼€å§‹åˆ†ææ—¶é—´ï¼Œç”¨äºåˆ†æå†å²æ•°æ®ï¼Œå¦‚æœ_analyze_start_timeè®¾ç½®æˆå½“å‰æ—¶é—´åˆ™ä»å½“å‰æ—¶é—´å¼€å§‹åˆ†æ
_boot_time = datetime.now(timezone.utc)
_analyze_start_time = datetime.now(timezone.utc)

# _group_id = "cc8f1d29-7a49-4975-95dc-7ac94aefc04b"
# _stop_analyze = False
# _analyze_start_time = parse_iso_time("2025-07-10T07:02:27")
def get_analyze_start_time() -> datetime:
    return _analyze_start_time + (datetime.now(timezone.utc) - _boot_time)


def feedback_setting(group_id: str, user_id: str, click_type: str, anomaly_analysis_results_id: str = None,
    detail_type: str = None, detail_status: str = None, share_to_user_ids: list = None):
    start_time = time.time()
    logger.info(f"ğŸ–±ï¸ [åé¦ˆç‚¹å‡»] æ”¶åˆ°ç”¨æˆ·{user_id}çš„{click_type}ç‚¹å‡»...")

    click_id = f"{group_id}_{user_id}_{int(datetime.now().timestamp())}"
    db.collection("feedback_clicks").document(click_id).set({
        "id": click_id,
        "group_id": group_id,
        "user_id": user_id,
        "click_type": click_type,
        "anomaly_analysis_results_id": anomaly_analysis_results_id,
        "clicked_at": datetime.now(timezone.utc).isoformat(),
        "detail_type": detail_type,
        "detail_status": detail_status,
        "share_to_user_ids": share_to_user_ids
    })
    logger.info(f"ğŸ’¾ [åé¦ˆç‚¹å‡»] å·²è®°å½•ç‚¹å‡»äº‹ä»¶åˆ°æ•°æ®åº“")

    # Lessæ—¶è°ƒæ•´è½®è¯¢å‘¨æœŸ
    if click_type == "Less":
        global _default_interval_seconds, _user_notify_interval_seconds
        old_interval = _user_notify_interval_seconds.get(user_id, _default_interval_seconds)

        if old_interval + 60 <= _default_interval_seconds * 2:
            _user_notify_interval_seconds[user_id] = old_interval + 60

        logger.info(f"ğŸ“Š [åé¦ˆç‚¹å‡»] æ£€æµ‹åˆ°Lessç‚¹å‡»ï¼Œè°ƒæ•´è½®è¯¢é—´éš”: {old_interval}s â†’ {old_interval+60}s")


    logger.info(f"âœ… [åé¦ˆç‚¹å‡»] å®Œæˆï¼è€—æ—¶{(time.time() - start_time):.2f}ç§’")
    return {"message": "åé¦ˆå·²è®°å½•"}


def start_analyze(group_id: str):
    global _group_id,_stop_analyze
    _stop_analyze = False
    _group_id = group_id

def stop_analyze(group_id: str):
    global _group_id,_stop_analyze
    _stop_analyze = True
    _group_id = group_id

def get_analyze_result():
    global _local_analyze_result_history,_ai_analyze_result_history
    return {"local":_local_analyze_result_history,"ai":_ai_analyze_result_history}

def analyze_handler():
    logger.info("ğŸ”„ [å¼‚å¸¸åˆ†æè½®è¯¢] å¯åŠ¨åˆ†æè½®è¯¢çº¿ç¨‹...")
    global _group_id, _stop_analyze, _local_analyze_result_history, _ai_analyze_q

    # 120ç§’çš„åˆ†æé—´éš”
    interval_seconds = 120
    last_analyze_time = None
    while True:
        try:
            if _stop_analyze:
                time.sleep(1)
                continue

            if _group_id is None:
                time.sleep(1)
                logger.error("âŒ [å¼‚å¸¸åˆ†æè½®è¯¢] group_idæœªè®¾ç½®ï¼Œæ— æ³•æ‰§è¡Œåˆ†æã€‚")
                continue


            if last_analyze_time is None:
                # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ‰§è¡Œï¼Œç­‰å¾…interval_secondså†å¼€å§‹
                last_analyze_time = get_analyze_start_time()
                time.sleep(interval_seconds)
            else:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç­‰å¾…ã€‚interval_secondsæ‰§è¡Œä¸€æ¬¡åˆ†æï¼Œä¸è¶³æ—¶é—´éœ€è¦ç­‰å¾…
                elapsed = get_analyze_start_time() - last_analyze_time
                if elapsed.total_seconds() < interval_seconds:
                    time.sleep(interval_seconds - elapsed.total_seconds())

            current_time = get_analyze_start_time()

            # æœ¬åœ°æ•°æ®åˆ†æ
            chunk_data_with_local_analyze, local_analyze_result = local_analyze(_group_id, last_analyze_time, current_time)

            # ç¼“å­˜æœ¬åœ°åˆ†æç»“æœ
            if local_analyze_result:
                _local_analyze_result_history.append({current_time.strftime("%Y-%m-%dT%H:%M:%S"): local_analyze_result})
                # åªä¿ç•™æœ€è¿‘20æ¬¡æ‰§è¡Œè®°å½•
                if len(_local_analyze_result_history) > 20:
                    _local_analyze_result_history = _local_analyze_result_history[-20:]


            # å†™å…¥é˜Ÿåˆ—ï¼Œå¼‚æ­¥å¤„ç†AIåˆ†æ
            if chunk_data_with_local_analyze:
                _ai_analyze_q.put((_group_id, chunk_data_with_local_analyze))

            # å–æœ¬æ¬¡åˆ†æçš„å¼€å§‹æ—¶é—´-å³å–æ•°æˆªæ­¢æ—¶é—´ï¼Œä½œä¸ºä¸‹ä¸€æ¬¡åˆ†æçš„èµ·å§‹æ—¶é—´
            last_analyze_time = current_time
        except Exception:
            logger.error('Error in analyze_handler loop: %s' % traceback.format_exc())


def ai_analyze_handler():
    logger.info("ğŸ”„ [AIå¼‚å¸¸åˆ†æ] å¯åŠ¨åˆ†æè½®è¯¢çº¿ç¨‹...")
    global  _stop_analyze, _ai_analyze_q, _ai_analyze_result_history

    while True:
        try:
            time.sleep(1)
            if _stop_analyze:
                continue

            while not _ai_analyze_q.empty():
                group_id, chunk_data_with_local_analyze = _ai_analyze_q.get()

                # è°ƒç”¨AIåˆ†ææ¥å£
                stage_start = time.time()
                result = None
                try:
                    result = asyncio.run(
                        ai_analyze_anomaly_status(group_id, chunk_data_with_local_analyze))
                    if not result:
                        # æ— åˆ†æç»“æœ
                        continue 

                    # ç¼“å­˜åˆ†æç»“æœ
                    end_time_str = chunk_data_with_local_analyze['time_range']['end']
                    _ai_analyze_result_history.append({end_time_str: result})
                    # åªä¿ç•™æœ€è¿‘20æ¬¡æ‰§è¡Œè®°å½•
                    if len(_ai_analyze_result_history) > 20:
                        _ai_analyze_result_history = _ai_analyze_result_history[-20:]
                except Exception as e:
                    logger.error(f"âŒ [AIå¼‚å¸¸åˆ†æ] AIåˆ†æå¼‚å¸¸: {traceback.format_exc()}")
                stage2_duration = time.time() - stage_start
                logger.info(f"ğŸ¤– [AIå¼‚å¸¸åˆ†æ] AIåˆ†æå®Œæˆï¼Œè€—æ—¶{stage2_duration:.2f}ç§’ã€‚ç»“æœï¼š", str(result)[:100])

        except Exception:
            logger.error('Error in ai_analyze_handler loop: %s' % traceback.format_exc())

def notify(user):
    global _ai_analyze_result_history

    if len(_ai_analyze_result_history) < 1:
        return

    user_id = user.get("user_id"),
    user_name = user.get("name", "æœªçŸ¥æˆå‘˜"),
    device_token = user.get("device_token", "")

    total_start_time = time.time()
    try:
        last_analyze_result = list(_ai_analyze_result_history[-1].values())[0]

        should_push = False
        glasses_summary = last_analyze_result.get("glasses_summary", "ä½ å½“å‰çŠ¶æ€éœ€è¦å…³æ³¨")
        score = last_analyze_result.get("score")

        # æ ¹æ®scoreçš„çŠ¶æ€è¯„åˆ†å’Œå†…å®¹ç›¸ä¼¼åº¦è¯„åˆ†åˆ¤æ–­æ˜¯å¦æ¨é€
        if score and isinstance(score, dict):
            state_score = score.get("state_score")
            content_similarity_score = score.get("content_similarity_score")
            if state_score is not None and content_similarity_score is not None:
                should_push = (state_score < 25 or state_score > 75) and (content_similarity_score < 50)
                logger.info(
                    f"ğŸ“Š [å¼‚å¸¸åˆ†æ] çŠ¶æ€è¯„åˆ†ï¼š{state_score}ï¼Œå†…å®¹ç›¸ä¼¼åº¦è¯„åˆ†ï¼š{content_similarity_score}ï¼Œæ¨é€é˜ˆå€¼ï¼šçŠ¶æ€è¯„åˆ†<25æˆ–>75ï¼Œå†…å®¹ç›¸ä¼¼åº¦è¯„åˆ†<50ï¼Œæ˜¯å¦æ¨é€ï¼š{should_push}")
            else:
                should_push = True  # å¦‚æœæ²¡æœ‰è¯„åˆ†ä¿¡æ¯ï¼Œé»˜è®¤æ¨é€
                logger.info(f"âš ï¸ [å¼‚å¸¸åˆ†æ] æœªæ‰¾åˆ°å®Œæ•´è¯„åˆ†ä¿¡æ¯ï¼Œé»˜è®¤æ¨é€")
        else:
            should_push = False  # å¦‚æœæ²¡æœ‰scoreä¿¡æ¯ï¼Œé»˜è®¤ä¸æ¨é€
            logger.info(f"âš ï¸ [å¼‚å¸¸åˆ†æ] æœªæ‰¾åˆ°è¯„åˆ†ä¿¡æ¯ï¼Œé»˜è®¤ä¸æ¨é€")


        # æ ¹æ®è¯„åˆ†å†³å®šæ˜¯å¦æ¨é€é€šçŸ¥
        if should_push and device_token:
            # JPush æ¨é€ - ä½¿ç”¨çœ¼é•œç‰ˆæœ¬
            send_jpush_notification(
                alert=glasses_summary,  # ç›´æ¥ä½¿ç”¨çœ¼é•œç‰ˆæœ¬ä½œä¸ºæ¨é€å†…å®¹
                registration_id=device_token,
                extras={
                    "type": "anomaly",
                    "title": "å¼‚å¸¸æé†’",
                    "body": glasses_summary,  # çœ¼é•œç‰ˆæœ¬
                    "summary": last_analyze_result.get("summary", "æš‚æ— æ‘˜è¦"),
                    "suggestion": last_analyze_result.get("detail", {}).get("suggestion", ""),
                    "user_id": user_id,
                    "user_name": user_name
                }
            )
            logger.info(f"âœ… [å¼‚å¸¸åˆ†æ] JPushæ¨é€å®Œæˆï¼Œç”¨æˆ· {user_name}({user_id})")
            logger.info(f"ğŸ“± [å¼‚å¸¸åˆ†æ] çœ¼é•œæ˜¾ç¤ºå†…å®¹ï¼š{glasses_summary}")
        elif not should_push:
            logger.info(f"â­ï¸ [å¼‚å¸¸åˆ†æ] è¯„åˆ†ä¸è¶³70åˆ†ï¼Œè·³è¿‡æ¨é€ï¼Œç”¨æˆ· {user_name}({user_id})")
        else:
            logger.info(f"âš ï¸ [å¼‚å¸¸åˆ†æ] ç”¨æˆ· {user_name}({user_id}) æœªæä¾› device_token")

        # WebSocket æ¨é€ - å‘PCé¡µé¢æ¨é€å®Œæ•´åˆ†æç»“æœï¼ˆæ— è®ºè¯„åˆ†å¦‚ä½•éƒ½æ¨é€ï¼‰
        try:
            push_anomaly_analysis_result(user_id, last_analyze_result)
        except Exception as e:
            logger.error(f"âš ï¸ [å¼‚å¸¸åˆ†æ] WebSocketæ¨é€å¤±è´¥: {e}")

    except Exception as e:
        logger.info(f"âŒ [AIå¼‚å¸¸é€šçŸ¥] user_id={user_id} å¯¼å…¥æˆ–è°ƒç”¨åˆ†ææ¥å£å¼‚å¸¸: {e}")

    total_duration = time.time() - total_start_time
    logger.info(f"âœ… [AIå¼‚å¸¸é€šçŸ¥] user_id={user_id} åˆ†æå®Œæˆï¼Œæ€»è€—æ—¶{total_duration:.2f}ç§’")


def notify_handler():
    logger.info("ğŸ”” [AIå¼‚å¸¸é€šçŸ¥è½®è¯¢] å¯åŠ¨é€šçŸ¥è½®è¯¢çº¿ç¨‹...")
    global _group_id,_stop_analyze,_default_interval_seconds,_user_notify_interval_seconds,_user_notify_last_time,_users_info

    while True:
        try:
            time.sleep(1)
            if _stop_analyze:
                continue

            if _users_info is None:
                continue

            for user in _users_info:
                user_id = user.get("user_id")

                last_notify_time = _user_notify_last_time.get(user_id)
                interval_seconds = _user_notify_interval_seconds.get(user_id,_default_interval_seconds)
                if last_notify_time is None:
                    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ‰§è¡Œï¼Œåˆå§‹åŒ–æ—¶é—´
                    _user_notify_last_time[user_id] = datetime.now(timezone.utc)
                    continue
                else:
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç­‰å¾…ã€‚interval_secondsæ‰§è¡Œä¸€æ¬¡é€šçŸ¥ï¼Œä¸è¶³æ—¶é—´ç»§ç»­è½®è¯¢ï¼Œç›´åˆ°è¶…è¿‡é—´éš”æ—¶é—´æ‰§è¡Œé€šçŸ¥
                    elapsed = datetime.now(timezone.utc) - last_notify_time
                    if elapsed.total_seconds() < interval_seconds:
                        continue

                    # æ‰§è¡Œé€šçŸ¥
                    notify(user)

                    _user_notify_last_time[user_id] = datetime.now(timezone.utc)

        except (Exception,):
            logger.error('Error in notify_handler loop: %s' % traceback.format_exc())

def run_analyze():
    global _analyze_thread, _ai_analyze_thread, _notify_thread
    try:
        _analyze_thread = threading.Thread(target=analyze_handler, name='_analyze_thread', daemon=True)
        _analyze_thread.start()

        _ai_analyze_thread = threading.Thread(target=ai_analyze_handler, name='_ai_analyze_thread', daemon=True)
        _ai_analyze_thread.start()
        #
        # # 5såå¯åŠ¨é€šçŸ¥çº¿ç¨‹ï¼Œé”™å¼€æ—¶é—´
        # time.sleep(5)
        #
        # _notify_thread = threading.Thread(target=notify_handler, name='_notify_thread', daemon=True)
        # _notify_thread.start()

    except (Exception,):
        logger.error('Error in notify_handler loop: %s' % traceback.format_exc())

if __name__ == "__main__":
    ...
    # run_analyze()
    # print(get_group_members_simple("0c90c6de-33e3-4431-b5fe-d06378111ef0"))

    # start_time = datetime.strptime("2025-07-09 02:45:00", "%Y-%m-%d %H:%M:%S")
    # end_time = datetime.strptime("2025-07-09 02:47:00", "%Y-%m-%d %H:%M:%S")
    # analyze("0c90c6de-33e3-4431-b5fe-d06378111ef0", start_time, end_time)


