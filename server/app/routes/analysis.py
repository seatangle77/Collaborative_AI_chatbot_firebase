import time
import traceback
from datetime import timedelta, datetime, timezone
from typing import Dict, Any, List

from fastapi import BackgroundTasks
from google.cloud.firestore_v1 import FieldFilter

from server.app.anomaly_analyze import local_analyze
from server.app.anomaly_polling_scheduler import start_analyze, stop_analyze
from server.app.anomaly_preprocessor import get_local_analyze_result, get_ai_analyze_result, \
    parse_iso_time
from server.app.database import db
from server.app.logger.logger_loader import logger
from server.app.websocket_routes import push_personal_share_message, push_anomaly_analysis_result

# æ¨¡æ‹Ÿç¼“å­˜ï¼šç”¨äºæš‚å­˜ interval summary ç»“æœ
interval_summary_cache: Dict[str, List[Dict[str, Any]]] = {}

# Firebase Admin SDK å¯¼å…¥ä¸åˆå§‹åŒ–
from firebase_admin import credentials, initialize_app
import firebase_admin

if not len(firebase_admin._apps):
    cred = credentials.Certificate("./server/firebase-key.json")
    initialize_app(cred)

from fastapi import APIRouter, Query
from pydantic import BaseModel
#
from server.app.jpush_api import jpush_personal_share_message, send_jpush_notification
import asyncio
from server.app.websocket_routes import push_stop_task, push_agenda_stage

router = APIRouter()

class IntervalSummaryRequest(BaseModel):
    group_id: str
    round_index: int
    start_time: str
    end_time: str

class GroupPollingRequest(BaseModel):
    group_id: str

class FeedbackClickRequest(BaseModel):
    group_id: str
    user_id: str
    click_type: str  # Less/More/Share
    anomaly_analysis_results_id: str = None
    detail_type: str = None
    detail_status: str = None
    share_to_user_ids: list = None

class PushAiAnalysisRequest(BaseModel):
    push_ji: bool
    device_token: str
    glasses_summary: str
    summary: str
    detail_suggestion: str
    user_id: str
    user_name: str
    push_pc: bool
    ai_analyze_result: dict


# 120ç§’çš„é—´éš”
_default_interval_seconds = 120
# ç”¨æˆ·å®šåˆ¶è‡ªå·±çš„æç¤ºé—´éš”æ—¶é—´ï¼Œ(user_id, interval_seconds)
_user_notify_interval_seconds = {}
# ç”¨æˆ·æœ€åä¸€æ¬¡é€šçŸ¥æ—¶é—´
_user_notify_last_time = {}

# æ›¿æ¢ä¸º POST æ–¹æ³•ï¼Œå‚æ•°ç»“æ„åŒ IntervalSummaryRequestï¼Œé€šè¿‡è¯·æ±‚ä½“æ¥æ”¶
@router.post("/analysis/anomalies")
async def get_anomaly_status(req: GroupPollingRequest):
    return get_local_analyze_result(req.group_id)

@router.post("/analysis/get_ai_analyze")
async def get_ai_analyze(req: GroupPollingRequest):
    return get_ai_analyze_result(req.group_id)

@router.post("/analysis/get_next_notify_ai_analyze_result")
async def get_next_notify_ai_analyze_result(req: GroupPollingRequest):
    global _user_notify_last_time, _user_notify_interval_seconds, _default_interval_seconds

    analyze_result = get_ai_analyze_result(req.group_id, limit=1)
    if len(analyze_result) > 0:
        last_ai_analyze_content = analyze_result[0]

        user_ids = list(last_ai_analyze_content.keys())
        end_time = last_ai_analyze_content.get("time_range",{}).get("end","")
        for user_id in user_ids:
            if user_id != "time_range":
                last_notify_time = _user_notify_last_time.get(user_id)
                interval_seconds = _user_notify_interval_seconds.get(user_id, _default_interval_seconds)
                if last_notify_time is None:
                    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ‰§è¡Œï¼Œç”¨AIåˆ†æç»“æœçš„æ—¶é—´ä½œä¸ºä¸‹æ¬¡é€šçŸ¥æ—¶é—´
                    next_notify_time = parse_iso_time(end_time)
                    last_ai_analyze_content.get(user_id, {}).update({"last_notify_time": ""})
                else:
                    # ä¸Šä¸€æ¬¡é€šçŸ¥æ—¶é—´+é—´éš”æ—¶é—´ä½œä¸ºä¸‹ä¸€æ¬¡é€šçŸ¥æ—¶é—´
                    next_notify_time = last_notify_time + timedelta(seconds=interval_seconds)
                    last_ai_analyze_content.get(user_id, {}).update({"last_notify_time": last_notify_time.isoformat()})

                last_ai_analyze_content.get(user_id, {}).update({"next_notify_time": next_notify_time.isoformat()})
                
        return last_ai_analyze_content
    else:
        return {}


@router.post("/analysis/push_ai_analyze_result")
async def push_ai_analyze_result(req: PushAiAnalysisRequest):
    global _user_notify_last_time
    _user_notify_last_time[req.user_id] = datetime.now(timezone.utc)

    # æ ¹æ®è¯„åˆ†å†³å®šæ˜¯å¦æ¨é€é€šçŸ¥
    try:
        if req.push_ji:
            # JPush æ¨é€ - ä½¿ç”¨çœ¼é•œç‰ˆæœ¬
            send_jpush_notification(
                alert=req.glasses_summary,  # ç›´æ¥ä½¿ç”¨çœ¼é•œç‰ˆæœ¬ä½œä¸ºæ¨é€å†…å®¹
                registration_id=req.device_token,
                extras={
                    "type": "anomaly",
                    "title": "å¼‚å¸¸æé†’",
                    "body": req.glasses_summary,  # çœ¼é•œç‰ˆæœ¬
                    "summary": req.summary,
                    "suggestion": req.detail_suggestion,
                    "user_id": req.user_id,
                    "user_name": req.user_name
                }
            )
            logger.info(f"âœ… [å¼‚å¸¸åˆ†æ] JPushæ¨é€å®Œæˆï¼Œç”¨æˆ· {req.user_name}({req.user_id}) çœ¼é•œæ˜¾ç¤ºå†…å®¹ï¼š{req.glasses_summary}")
    except Exception as e:
        logger.error(f"âš ï¸ [å¼‚å¸¸åˆ†æ] JPushæ¨é€å¤±è´¥: {traceback.format_exc()}")

    # WebSocket æ¨é€ - å‘PCé¡µé¢æ¨é€å®Œæ•´åˆ†æç»“æœ
    try:
        if req.push_pc:
            logger.info(f"ğŸ“¡ [å¼‚å¸¸åˆ†æ] å¼€å§‹WebSocketæ¨é€ï¼Œç”¨æˆ·ID: {req.user_id}")
            await push_anomaly_analysis_result(req.user_id, req.ai_analyze_result)
            logger.info(f"âœ… [å¼‚å¸¸åˆ†æ] WebSocketæ¨é€å®Œæˆï¼Œç”¨æˆ·ID: {req.user_id}")
        else:
            logger.info(f"â­ï¸ [å¼‚å¸¸åˆ†æ] è·³è¿‡WebSocketæ¨é€ï¼Œpush_pc: {req.push_pc}")
    except Exception as e:
        logger.error(f"âš ï¸ [å¼‚å¸¸åˆ†æ] WebSocketæ¨é€å¤±è´¥: {traceback.format_exc()}")

    # æ„é€ GroupPollingRequestæ¥è°ƒç”¨get_next_notify_ai_analyze_result
    # ä»ai_analyze_resultä¸­æå–group_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
    group_id = req.ai_analyze_result.get('group_id', 'default_group')
    polling_req = GroupPollingRequest(group_id=group_id)
    return await get_next_notify_ai_analyze_result(polling_req)

# æ–°å¢ï¼šåªæ‰§è¡Œæœ¬åœ°åˆ†æçš„æ¥å£
@router.post("/analysis/local_anomalies")
async def get_realtime_local_anomaly_status(req: IntervalSummaryRequest):
    """
    åªæ‰§è¡Œæœ¬åœ°åˆ†æï¼Œä¸è°ƒç”¨AIåˆ†æï¼Œç›´æ¥è¿”å›æœ¬åœ°åˆ†æç»“æœ
    """
    try:

        start_time = time.time()
        logger.info(f"ğŸ” [æœ¬åœ°å¼‚å¸¸åˆ†æ] å¼€å§‹åˆ†æ group_id={req.group_id}")
        
        # æœ¬åœ°æ•°æ®åˆ†æ
        chunk_data_with_local_analyze, local_analyze_result = local_analyze(req.group_id, req.start_time, req.end_time, is_save_debug_info=False)

        processing_time = time.time() - start_time
        logger.info(f"ğŸ“Š [æœ¬åœ°å¼‚å¸¸åˆ†æ] åˆ†æå®Œæˆï¼Œè€—æ—¶{processing_time:.2f}ç§’")

        # ç¼“å­˜æœ¬åœ°åˆ†æç»“æœ
        if local_analyze_result:
            return {
                "local_analysis": local_analyze_result,
                "raw_data_summary": {
                    "time_range": chunk_data_with_local_analyze.get('time_range'),
                    "users_count": len(chunk_data_with_local_analyze.get('users', [])),
                    "speech_transcripts_count": len(chunk_data_with_local_analyze.get('raw_tables', {}).get('speech_transcripts', [])),
                    "note_edit_history_count": len(chunk_data_with_local_analyze.get('raw_tables', {}).get('note_edit_history', [])),
                    "pageBehaviorLogs_count": len(chunk_data_with_local_analyze.get('raw_tables', {}).get('pageBehaviorLogs', {}))
                },
                "processing_time": processing_time,
                "analysis_timestamp": time.time()
            }
        else:
            return {
                "local_analysis": None,
                "message": "ç”¨æˆ·æ´»åŠ¨æ•°æ®å¢é‡ä¸º0ï¼Œæ— æ³•è¿›è¡Œåˆ†æ",
                "processing_time": time.time() - start_time
            }
        
    except Exception as e:
        logger.error(f"âŒ [æœ¬åœ°å¼‚å¸¸åˆ†æ] åˆ†æå¤±è´¥: {traceback.format_exc()}")
        return {
            "error": str(e),
            "local_analysis": None,
            "processing_time": time.time() - start_time if 'start_time' in locals() else 0
        }


@router.get("/analysis/anomaly_results_by_user")
async def get_anomaly_results_by_user(
    group_id: str,
    user_id: str = Query(..., description="ç”¨æˆ·ID"),
    page: int = Query(1, ge=1, description="é¡µç ï¼Œä»1å¼€å§‹"),
    page_size: int = Query(10, ge=1, le=100, description="æ¯é¡µæ¡æ•°ï¼Œæœ€å¤§100")
):

    try:
        # æ³¨æ„ï¼šç”±äºæ•°æ®åº“ä¸­å®é™…æ²¡æœ‰å­˜å‚¨ current_user å­—æ®µï¼Œè¿™é‡Œæš‚æ—¶æŸ¥è¯¢æ‰€æœ‰è®°å½•
        results = db.collection("anomaly_analysis_group_results") \
            .where(filter=FieldFilter("group_id", "==", group_id)) \
            .order_by("created_at", direction="DESCENDING") \
            .stream()

        # è·å–æ€»æ•°
        ai_result_list = [doc.to_dict() for doc in results]
        total = len(ai_result_list)

        # è®¡ç®—åç§»é‡
        offset = (page - 1) * page_size

        # è·å–åˆ†é¡µæ•°æ®
        paginated_docs = ai_result_list[offset:offset + page_size]
        return_list = []
        for doc in paginated_docs:
            raw_response = doc.get("raw_response", {})
            if isinstance(raw_response, dict):
                user_anomaly_result = raw_response.get(user_id, {})
                # æ·»åŠ æ•°æ®åº“è®°å½•çš„ID
                user_anomaly_result["record_id"] = doc.get("id", "")
                user_anomaly_result["start_time"] = doc.get("start_time", "")
                user_anomaly_result["end_time"] = doc.get("end_time", "")
                return_list.append(user_anomaly_result)

        return {
            "results": return_list,
            "total": total,
            "page": page,
            "page_size": page_size,
            "total_pages": (total + page_size - 1) // page_size
        }
        
    except Exception as e:
        logger.info(f"æŸ¥è¯¢å¼‚å¸¸åˆ†æç»“æœå¤±è´¥: {e}")
        # è¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        return {
            "results": [],
            "total": 0,
            "page": page,
            "page_size": page_size,
            "total_pages": 0
        }


@router.post("/analysis/anomaly_polling/start")
async def start_anomaly_polling(req: GroupPollingRequest):
    global _user_notify_interval_seconds, _user_notify_last_time
    _user_notify_interval_seconds = {}
    _user_notify_last_time = {}

    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(None, start_analyze, req.group_id)
    await push_agenda_stage(req.group_id, 1)
    return result


@router.post("/analysis/anomaly_polling/stop")
async def stop_anomaly_polling(req: GroupPollingRequest):
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(None, stop_analyze, req.group_id)
    await push_stop_task(req.group_id)
    return result

@router.post("/analysis/anomaly_polling/feedback_click")
def feedback_click(req: FeedbackClickRequest, background_tasks: BackgroundTasks):
    start_time = time.time()
    logger.info(f"ğŸ–±ï¸ [åé¦ˆç‚¹å‡»] æ”¶åˆ°ç”¨æˆ·{req.user_id}çš„{req.click_type}ç‚¹å‡»...")

    # Less\More æ—¶è°ƒæ•´è½®è¯¢å‘¨æœŸ
    global _default_interval_seconds, _user_notify_interval_seconds
    if req.click_type == "Less":
        old_interval = _user_notify_interval_seconds.get(req.user_id, _default_interval_seconds)

        if old_interval + 60 <= _default_interval_seconds * 2:
            _user_notify_interval_seconds[req.user_id] = old_interval + 60

        logger.info(
            f"ğŸ“Š [åé¦ˆç‚¹å‡»] æ£€æµ‹åˆ°Lessç‚¹å‡»ï¼Œè°ƒæ•´è½®è¯¢é—´éš”: {old_interval}s â†’ {_user_notify_interval_seconds.get(req.user_id, _default_interval_seconds)}s")
    elif req.click_type == "More":
        old_interval = _user_notify_interval_seconds.get(req.user_id, _default_interval_seconds)

        if old_interval - 60 >= _default_interval_seconds:
            _user_notify_interval_seconds[req.user_id] = old_interval - 60

        logger.info(
            f"ğŸ“Š [åé¦ˆç‚¹å‡»] æ£€æµ‹åˆ°Moreç‚¹å‡»ï¼Œè°ƒæ•´è½®è¯¢é—´éš”: {old_interval}s â†’ {_user_notify_interval_seconds.get(req.user_id, _default_interval_seconds)}s")
    # Shareæ—¶é€šè¿‡WebSocketæ¨é€
    elif req.click_type == "Share" and req.share_to_user_ids:
        logger.info(f"ğŸ“¤ [åé¦ˆç‚¹å‡»] æ£€æµ‹åˆ°Shareç‚¹å‡»ï¼Œå‡†å¤‡æ¨é€æ¶ˆæ¯...")

        for uid in req.share_to_user_ids:
            background_tasks.add_task(
                push_personal_share_message,
                user_id=uid,
                from_user=req.user_id,
                detail_type=req.detail_type,
                detail_status=req.detail_status,
                group_id=req.group_id
            )
            background_tasks.add_task(
                jpush_personal_share_message,
                user_id=uid,
                from_user=req.user_id,
                detail_type=req.detail_type,
                detail_status=req.detail_status,
                group_id=req.group_id
            )

        logger.info(f"ğŸ“¤ [åé¦ˆç‚¹å‡»] å·²æ·»åŠ {len(req.share_to_user_ids)}ä¸ªæ¨é€ä»»åŠ¡")

    click_id = f"{req.group_id}_{req.user_id}_{int(datetime.now().timestamp())}"
    feedback_setting = {
        "id": click_id,
        "group_id": req.group_id,
        "user_id": req.user_id,
        "click_type": req.click_type,
        "anomaly_analysis_results_id": req.anomaly_analysis_results_id,
        "clicked_at": datetime.now(timezone.utc).isoformat(),
        "detail_type": req.detail_type,
        "detail_status": req.detail_status,
        "share_to_user_ids": req.share_to_user_ids,
        "interval_seconds": _user_notify_interval_seconds.get(req.user_id, _default_interval_seconds),
    }
    db.collection("feedback_clicks").document(click_id).set(feedback_setting)

    logger.info(f"âœ… [åé¦ˆç‚¹å‡»] å®Œæˆï¼è€—æ—¶{(time.time() - start_time):.2f}ç§’")
    return {"message": "åé¦ˆå·²è®°å½•", "feedback_setting": feedback_setting}


