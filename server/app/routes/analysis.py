from fastapi import Body
from typing import Dict, Any, List
# æ¨¡æ‹Ÿç¼“å­˜ï¼šç”¨äºæš‚å­˜ interval summary ç»“æœ
interval_summary_cache: Dict[str, List[Dict[str, Any]]] = {}

# Firebase Admin SDK å¯¼å…¥ä¸åˆå§‹åŒ–
from firebase_admin import messaging, credentials, initialize_app
import firebase_admin
import os
if not len(firebase_admin._apps):
    cred = credentials.Certificate("./server/firebase-key.json")
    initialize_app(cred)

from fastapi import APIRouter, Query, HTTPException
import json
from pydantic import BaseModel
#
from app.preprocessor_summary import extract_chunk_data
from app.preprocessor_anomaly import extract_chunk_data_anomaly
from app.analyze_chunk_with_ai import analyze_cognitive, analyze_behavior, analyze_attention
from app.analyze_chunk_with_ai_anomaly import analyze_all_anomalies
from app.jpush_api import send_jpush_notification

router = APIRouter()

class Member(BaseModel):
    id: str
    name: str

class CurrentUser(BaseModel):
    user_id: str
    name: str
    device_token: str

class IntervalSummaryRequest(BaseModel):
    group_id: str
    round_index: int
    start_time: str
    end_time: str
    members: List[Member]
    current_user: CurrentUser  # å‰ç«¯ç›´æ¥ä¼ å…¥å½“å‰ç”¨æˆ·ä¿¡æ¯


# æ›¿æ¢ä¸º POST æ–¹æ³•ï¼Œå‚æ•°ç»“æ„åŒ IntervalSummaryRequestï¼Œé€šè¿‡è¯·æ±‚ä½“æ¥æ”¶
@router.post("/analysis/anomalies")
async def get_anomaly_status(req: IntervalSummaryRequest):
    import time
    total_start_time = time.time()
    print(f"ğŸš€ [å¼‚å¸¸åˆ†æ] å¼€å§‹åˆ†ægroup_id={req.group_id}ï¼Œç”¨æˆ·={req.current_user.name}...")
    
    members = [{"user_id": m.id, "name": m.name} for m in req.members]

    # é˜¶æ®µ1: æ•°æ®é¢„å¤„ç†
    stage1_start = time.time()
    raw_data = extract_chunk_data_anomaly(
        group_id=req.group_id,
        round_index=req.round_index,
        start_time=req.start_time,
        end_time=req.end_time,
        member_list=members,
        current_user=req.current_user.dict()
    )
    stage1_duration = time.time() - stage1_start
    print(f"ğŸ“Š [å¼‚å¸¸åˆ†æ] é˜¶æ®µ1-æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶{stage1_duration:.2f}ç§’")
    
    # é˜¶æ®µ2: AIåˆ†æ
    stage2_start = time.time()
    result = analyze_all_anomalies(raw_data)
    stage2_duration = time.time() - stage2_start
    print(f"ğŸ¤– [å¼‚å¸¸åˆ†æ] é˜¶æ®µ2-AIåˆ†æå®Œæˆï¼Œè€—æ—¶{stage2_duration:.2f}ç§’")
    
    # é˜¶æ®µ3: ç»“æœè§£æ
    stage3_start = time.time()
    # è§£æAIè¿”å›çš„JSONç»“æœ
    import re
    summary = None
    glasses_summary = None
    detail = None
    user_data_summary = None
    more_info = None
    score = None
    should_push = False
    try:
        if isinstance(result.get("raw_response"), str):
            raw = result["raw_response"]
            # ç”¨æ­£åˆ™æå–å‡º {...} éƒ¨åˆ†
            match = re.search(r"{[\s\S]*}", raw)
            if match:
                json_str = match.group(0)
                parsed_result = json.loads(json_str)
                summary = parsed_result.get("summary")
                glasses_summary = parsed_result.get("glasses_summary", "ä½ å½“å‰çŠ¶æ€éœ€è¦å…³æ³¨")
                detail = parsed_result.get("detail")
                user_data_summary = parsed_result.get("user_data_summary")
                more_info = parsed_result.get("more_info")
                score = parsed_result.get("score")
                
                # æ ¹æ®scoreçš„çŠ¶æ€è¯„åˆ†å’Œå†…å®¹ç›¸ä¼¼åº¦è¯„åˆ†åˆ¤æ–­æ˜¯å¦æ¨é€
                if score and isinstance(score, dict):
                    state_score = score.get("state_score")
                    content_similarity_score = score.get("content_similarity_score")
                    should_push = False
                    if state_score is not None and content_similarity_score is not None:
                        should_push = (state_score < 25 or state_score > 75) or (content_similarity_score < 50)
                        print(f"ğŸ“Š [å¼‚å¸¸åˆ†æ] çŠ¶æ€è¯„åˆ†ï¼š{state_score}ï¼Œå†…å®¹ç›¸ä¼¼åº¦è¯„åˆ†ï¼š{content_similarity_score}ï¼Œæ¨é€é˜ˆå€¼ï¼šçŠ¶æ€è¯„åˆ†<25æˆ–>75ï¼Œå†…å®¹ç›¸ä¼¼åº¦è¯„åˆ†<50ï¼Œæ˜¯å¦æ¨é€ï¼š{should_push}")
                    else:
                        should_push = True  # å¦‚æœæ²¡æœ‰è¯„åˆ†ä¿¡æ¯ï¼Œé»˜è®¤æ¨é€
                        print(f"âš ï¸ [å¼‚å¸¸åˆ†æ] æœªæ‰¾åˆ°å®Œæ•´è¯„åˆ†ä¿¡æ¯ï¼Œé»˜è®¤æ¨é€")
                else:
                    should_push = False  # å¦‚æœæ²¡æœ‰scoreä¿¡æ¯ï¼Œé»˜è®¤ä¸æ¨é€
                    print(f"âš ï¸ [å¼‚å¸¸åˆ†æ] æœªæ‰¾åˆ°è¯„åˆ†ä¿¡æ¯ï¼Œé»˜è®¤ä¸æ¨é€")
            else:
                glasses_summary = "ä½ å½“å‰çŠ¶æ€éœ€è¦å…³æ³¨"
                should_push = True
        else:
            glasses_summary = "ä½ å½“å‰çŠ¶æ€éœ€è¦å…³æ³¨"
            should_push = True
    except Exception as e:
        print("è§£æAIå“åº”å¤±è´¥ï¼š", e)
        glasses_summary = "ä½ å½“å‰çŠ¶æ€éœ€è¦å…³æ³¨"
        should_push = True
    stage3_duration = time.time() - stage3_start
    print(f"ğŸ“ [å¼‚å¸¸åˆ†æ] é˜¶æ®µ3-ç»“æœè§£æå®Œæˆï¼Œè€—æ—¶{stage3_duration:.2f}ç§’")
    
    # é˜¶æ®µ4: æ–‡ä»¶å­˜å‚¨
    stage4_start = time.time()
    # ä¿å­˜åˆ†æç»“æœä¸ºæ–‡ä»¶
    import uuid
    from datetime import datetime
    from datetime import timezone
    os.makedirs("analysis_outputs", exist_ok=True)
    file_name = f"analysis_outputs/anomaly_{uuid.uuid4()}_{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}.json"
    with open(file_name, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    stage4_duration = time.time() - stage4_start
    print(f"ğŸ’¾ [å¼‚å¸¸åˆ†æ] é˜¶æ®µ4-æ–‡ä»¶å­˜å‚¨å®Œæˆï¼Œè€—æ—¶{stage4_duration:.2f}ç§’")

    # é˜¶æ®µ5: æ•°æ®åº“å­˜å‚¨
    stage5_start = time.time()
    # æ–°å»º anomaly_analysis_files è¡¨å¹¶æ’å…¥å†…å®¹
    from app.database import db
    file_id = str(uuid.uuid4())
    db.collection("anomaly_raw_json_input").document(file_id).set({
        "id": file_id,
        "group_id": req.group_id,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "raw_json": result  # å®Œæ•´åˆ†æå†…å®¹
    })

    # æ–°å»ºanomaly_analysis_resultsè¡¨å¹¶æ’å…¥æ•°æ®
    analysis_id = str(uuid.uuid4())
    db.collection("anomaly_analysis_results").document(analysis_id).set({
        "id": analysis_id,
        "group_id": req.group_id,
        "start_time": req.start_time,
        "end_time": req.end_time,
        "current_user": req.current_user.dict(),
        "raw_response": result.get("raw_response"),
        "summary": summary,
        "glasses_summary": glasses_summary,
        "detail": detail,
        "user_data_summary": user_data_summary,
        "more_info": more_info,
        "score": score,
        "should_push": should_push,
        "created_at": datetime.now(timezone.utc).isoformat()
    })
    stage5_duration = time.time() - stage5_start
    print(f"ğŸ—„ï¸ [å¼‚å¸¸åˆ†æ] é˜¶æ®µ5-æ•°æ®åº“å­˜å‚¨å®Œæˆï¼Œè€—æ—¶{stage5_duration:.2f}ç§’")

    # é˜¶æ®µ6: æ¨é€é€šçŸ¥
    stage6_start = time.time()
    # ä½¿ç”¨å‰ç«¯ä¼ å…¥çš„å½“å‰ç”¨æˆ·ä¿¡æ¯å‘é€æ¨é€é€šçŸ¥
    current_user = req.current_user
    device_token = current_user.device_token
    
    # æ„é€ è¿”å›ç»™å‰ç«¯çš„æ•°æ®
    response_data = {
        "raw_response": result.get("raw_response"),
        "summary": summary,
        "glasses_summary": glasses_summary,
        "detail": detail,
        "user_data_summary": user_data_summary,
        "more_info": more_info,
        "score": score,
        "should_push": should_push,
        "current_user": req.current_user.dict(),
        "group_id": req.group_id,
        "start_time": req.start_time,
        "end_time": req.end_time,
        "analysis_id": analysis_id,
        "anomaly_analysis_results_id": analysis_id  # æ·»åŠ å…¼å®¹å­—æ®µ
    }
    
    print(f"ğŸ” [è°ƒè¯•] æ¨é€æ•°æ®ä¸­çš„IDå­—æ®µ:")
    print(f"  - analysis_id: {analysis_id}")
    print(f"  - anomaly_analysis_results_id: {analysis_id}")
    
    # æ ¹æ®è¯„åˆ†å†³å®šæ˜¯å¦æ¨é€é€šçŸ¥
    if should_push and device_token:
        # JPush æ¨é€ - ä½¿ç”¨çœ¼é•œç‰ˆæœ¬
        send_jpush_notification(
            alert=glasses_summary,  # ç›´æ¥ä½¿ç”¨çœ¼é•œç‰ˆæœ¬ä½œä¸ºæ¨é€å†…å®¹
            registration_id=device_token,
            extras={
                "type": "anomaly",
                "title": "å¼‚å¸¸æé†’",
                "body": glasses_summary,  # çœ¼é•œç‰ˆæœ¬
                "summary": summary or result.get("summary", "æš‚æ— æ‘˜è¦"),
                "suggestion": (detail or {}).get("suggestion", "") if detail else result.get("detail", {}).get("suggestion", ""),
                "user_id": current_user.user_id,
                "user_name": current_user.name
            }
        )
        print(f"âœ… [å¼‚å¸¸åˆ†æ] JPushæ¨é€å®Œæˆï¼Œç”¨æˆ· {current_user.name}({current_user.user_id})")
        print(f"ğŸ“± [å¼‚å¸¸åˆ†æ] çœ¼é•œæ˜¾ç¤ºå†…å®¹ï¼š{glasses_summary}")
    elif not should_push:
        print(f"â­ï¸ [å¼‚å¸¸åˆ†æ] è¯„åˆ†ä¸è¶³70åˆ†ï¼Œè·³è¿‡æ¨é€ï¼Œç”¨æˆ· {current_user.name}({current_user.user_id})")
    else:
        print(f"âš ï¸ [å¼‚å¸¸åˆ†æ] ç”¨æˆ· {current_user.name}({current_user.user_id}) æœªæä¾› device_token")

    # WebSocket æ¨é€ - å‘PCé¡µé¢æ¨é€å®Œæ•´åˆ†æç»“æœï¼ˆæ— è®ºè¯„åˆ†å¦‚ä½•éƒ½æ¨é€ï¼‰
    try:
        from app.websocket_routes import push_anomaly_analysis_result
        await push_anomaly_analysis_result(current_user.user_id, response_data)
        print(f"ğŸ“¡ [å¼‚å¸¸åˆ†æ] WebSocketæ¨é€å®Œæˆï¼Œç”¨æˆ· {current_user.name}({current_user.user_id})")
    except Exception as e:
        print(f"âš ï¸ [å¼‚å¸¸åˆ†æ] WebSocketæ¨é€å¤±è´¥: {e}")
    
    stage6_duration = time.time() - stage6_start
    print(f"ğŸ“¤ [å¼‚å¸¸åˆ†æ] é˜¶æ®µ6-æ¨é€é€šçŸ¥å®Œæˆï¼Œè€—æ—¶{stage6_duration:.2f}ç§’")

    total_duration = time.time() - total_start_time
    print(f"âœ… [å¼‚å¸¸åˆ†æ] group_id={req.group_id}ï¼Œç”¨æˆ·={req.current_user.name}åˆ†æå®Œæˆï¼Œæ€»è€—æ—¶{total_duration:.2f}ç§’")

    # è¿”å›ç»™å‰ç«¯æ›´å¤šä¿¡æ¯
    return {
        "raw_response": result.get("raw_response"),
        "summary": summary,
        "glasses_summary": glasses_summary,
        "detail": detail,
        "user_data_summary": user_data_summary,
        "more_info": more_info,
        "score": score,
        "should_push": should_push,
        "current_user": req.current_user.dict(),
        "group_id": req.group_id,
        "start_time": req.start_time,
        "end_time": req.end_time,
        "analysis_id": analysis_id,
        "anomaly_analysis_results_id": analysis_id  # æ·»åŠ å…¼å®¹å­—æ®µ
    }


@router.post("/analysis/interval_summary")
async def interval_summary(req: IntervalSummaryRequest):
    """
    æ¯5åˆ†é’Ÿè‡ªåŠ¨åˆ†æä¸€æ¬¡ï¼Œå°†æ®µè½åˆ†æç»“æœæš‚å­˜ï¼ˆæ¨¡æ‹Ÿç¼“å­˜ï¼‰ã€‚
    """
    import uuid
    from datetime import datetime
    import os

    members = [{"user_id": m.id, "name": m.name} for m in req.members]

    # é€šè¿‡ extract_chunk_data è·å– chunk æ•°æ®
    chunk_data = extract_chunk_data(
        group_id=req.group_id,
        round_index=req.round_index,
        start_time=req.start_time,
        end_time=req.end_time,
        member_list=members
    )
    #print("ğŸ§ª chunk_data result:", chunk_data)

    cog_result = analyze_cognitive(chunk_data)
    beh_result = analyze_behavior(chunk_data)
    attn_result = analyze_attention(chunk_data)

    os.makedirs("analysis_outputs", exist_ok=True)
    result = {
        "cognitive": cog_result,
        "behavior": beh_result,
        "attention": attn_result
    }
    file_name = f"analysis_outputs/interval_{uuid.uuid4()}_{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}.json"
    with open(file_name, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    return result


@router.get("/analysis/round_summary_combined")
async def round_summary_combined(
    group_id: str = Query(...),
    round_index: int = Query(...),
    start_time: str = Query(...),
    end_time: str = Query(...)
):
    """
    å½“å‰è½®ç»“æŸåæ‰‹åŠ¨è§¦å‘ï¼Œå°†æ‰€æœ‰ interval summary æ±‡æ€»æˆå®Œæ•´ä¸€è½®æ€»ç»“ã€‚
    """
    cache_key = f"{group_id}:{round_index}"
    interval_chunks = interval_summary_cache.get(cache_key, [])
    # æŒ‰ç”¨æˆ·åˆ†ç»„ï¼Œæ”¶é›†æ‰€æœ‰chunkçš„summary
    user_chunks = {}
    for chunk in interval_chunks:
        uid = chunk["user_id"]
        if uid not in user_chunks:
            user_chunks[uid] = []
        user_chunks[uid].append(chunk["summary"])

    members = []
    for user_id, summaries in user_chunks.items():
        # å¯ä»¥è¿›ä¸€æ­¥åˆå¹¶æ¯ä¸ªsummaryçš„levelï¼ˆå–ä¼—æ•°æˆ–å¹³å‡ç­‰ï¼‰
        behavioral_levels = [s.get("behavioral_level") for s in summaries]
        cognitive_levels = [s.get("cognitive_level") for s in summaries]
        attention_levels = [s.get("shared_attention") for s in summaries]

        def most_common_level(levels):
            return max(set(levels), key=levels.count) if levels else "æœªçŸ¥"

        # æ±‡æ€»
        member_summary = {
            "time_range": {
                "start": start_time,
                "end": end_time
            },
            "user_id": user_id,
            "round": round_index,
            "behavioral_level": most_common_level(behavioral_levels),
            "cognitive_level": most_common_level(cognitive_levels),
            "shared_attention": most_common_level(attention_levels),
            "suggestions": [
                "è¯·å°è¯•æé«˜è®¤çŸ¥äº’åŠ¨æ°´å¹³",
                "æ˜¯å¦å¯ä»¥è¿›ä¸€æ­¥èšç„¦ç›¸ä¼¼é¡µé¢"
            ],
            "anomalies": []
        }
        members.append(member_summary)

    # æ±‡æ€» group
    all_behaviors = [m["behavioral_level"] for m in members]
    all_cognition = [m["cognitive_level"] for m in members]
    all_attention = [m["shared_attention"] for m in members]
    def most_common_level(levels):
        return max(set(levels), key=levels.count) if levels else "æœªçŸ¥"
    group_summary = {
        "time_range": {
            "start": start_time,
            "end": end_time
        },
        "user_id": "group",
        "round": round_index,
        "behavioral_level": most_common_level(all_behaviors),
        "cognitive_level": most_common_level(all_cognition),
        "shared_attention": most_common_level(all_attention),
        "suggestions": [
            "è¯·å°è¯•æé«˜è®¤çŸ¥äº’åŠ¨æ°´å¹³",
            "æ˜¯å¦å¯ä»¥è¿›ä¸€æ­¥èšç„¦ç›¸ä¼¼é¡µé¢"
        ]
    }

    return {
        "group_summary": group_summary,
        "members": members
    }

@router.get("/analysis/anomaly_results_by_user")
async def get_anomaly_results_by_user(
    user_id: str = Query(..., description="ç”¨æˆ·ID"),
    page: int = Query(1, ge=1, description="é¡µç ï¼Œä»1å¼€å§‹"),
    page_size: int = Query(10, ge=1, le=100, description="æ¯é¡µæ¡æ•°ï¼Œæœ€å¤§100")
):
    from app.database import db
    
    try:
        # å…ˆè·å–æ‰€æœ‰åŒ¹é…çš„æ–‡æ¡£ï¼ˆä¸æ’åºï¼‰
        base_query = db.collection("anomaly_analysis_results") \
            .where("current_user.user_id", "==", user_id)
        
        # è·å–æ€»æ•°
        total_docs = list(base_query.stream())
        total = len(total_docs)
        
        # åœ¨å†…å­˜ä¸­æ’åºå’Œåˆ†é¡µ
        sorted_docs = sorted(total_docs, key=lambda doc: doc.get("created_at") or "", reverse=True)
        
        # è®¡ç®—åç§»é‡
        offset = (page - 1) * page_size
        
        # è·å–åˆ†é¡µæ•°æ®
        paginated_docs = sorted_docs[offset:offset + page_size]
        data = [doc.to_dict() for doc in paginated_docs]
        
        return {
            "results": data,
            "total": total,
            "page": page,
            "page_size": page_size,
            "total_pages": (total + page_size - 1) // page_size
        }
        
    except Exception as e:
        print(f"æŸ¥è¯¢å¼‚å¸¸åˆ†æç»“æœå¤±è´¥: {e}")
        # è¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        return {
            "results": [],
            "total": 0,
            "page": page,
            "page_size": page_size,
            "total_pages": 0
        }