import copy
from collections import defaultdict
from typing import Tuple

from server.app.database import db
import os, json
from datetime import datetime
from datetime import timezone, timedelta
from google.cloud.firestore_v1.base_query import FieldFilter
import time

from server.app.logger.logger_loader import logger


def parse_iso_time(iso_str):
    if not iso_str:
        return None
    if isinstance(iso_str, dict) and "_seconds" in iso_str:
        # Firestore timestamp dict
        return datetime.fromtimestamp(
            iso_str["_seconds"] + iso_str.get("_nanoseconds", 0) / 1e9,
            tz=timezone.utc
        )
    if iso_str.endswith("Z"):
        iso_str = iso_str.replace("Z", "+00:00")
    try:
        dt = datetime.fromisoformat(iso_str)
        # å…³é”®å¤„ç†ï¼šæ— æ—¶åŒºæ ‡è¯†æ—¶è®¾ä¸ºä¸œå…«åŒº
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone(timedelta(hours=0)))
        return dt
    except Exception:
        return None

def merge_tab_behavior_logs(actions):
    """
    è¿ç»­åŒç±»å‹æ“ä½œåˆå¹¶ï¼Œè®°å½•å¼€å§‹æ—¶é—´ã€ç»“æŸæ—¶é—´ã€æ“ä½œæ¬¡æ•°ã€é¦–å°¾ä½ç½®ç­‰ã€‚
    mousemove: åˆå¹¶è¿ç»­ï¼Œä¿ç•™é¦–å°¾ä½ç½®å’Œæ—¶é—´ã€‚
    scroll: åˆå¹¶è¿ç»­ï¼Œä¿ç•™é¦–å°¾æ—¶é—´ã€æœ€å¤§/æœ€å°maxDepthã€‚
    click: åˆå¹¶è¿ç»­ï¼Œä¿ç•™é¦–å°¾ä½ç½®å’Œæ—¶é—´ã€æ¬¡æ•°ã€‚
    å…¶ä»–ç±»å‹ä¿æŒåŸæ ·ã€‚
    """
    if not actions:
        return []
    merged = []
    i = 0
    n = len(actions)
    while i < n:
        curr = actions[i]
        curr_type = curr.get('type')
        curr_tabid = curr.get('tabId')
        start_time = curr.get('timestamp')
        end_time = curr.get('timestamp')
        count = 1
        positions = []
        maxDepths = []
        if curr_type == 'mousemove' and 'position' in curr:
            positions.append(curr['position'])
        if curr_type == 'scroll' and 'maxDepth' in curr:
            maxDepths.append(curr['maxDepth'])
        if curr_type == 'click' and 'position' in curr:
            positions.append(curr['position'])

        # å‘åæŸ¥æ‰¾è¿ç»­çš„åŒç±»å‹æ“ä½œ
        j = i + 1
        while j < n and actions[j].get('type') == curr_type and actions[j].get('tabId') == curr_tabid:
            end_time = actions[j].get('timestamp')
            count += 1
            if curr_type == 'mousemove' and 'position' in actions[j]:
                positions.append(actions[j]['position'])
            if curr_type == 'scroll' and 'maxDepth' in actions[j]:
                maxDepths.append(actions[j]['maxDepth'])
            if curr_type == 'click' and 'position' in actions[j]:
                positions.append(actions[j]['position'])
            j += 1

        merged_item = {
            'type': curr_type,
            'startTime': start_time,
            'endTime': end_time,
            'action_count': count
        }
        if curr_type == 'mousemove' and positions:
            merged_item['startPosition'] = positions[0]
            merged_item['endPosition'] = positions[-1]
        if curr_type == 'scroll' and maxDepths:
            merged_item['minDepth'] = min(maxDepths)
            merged_item['maxDepth'] = max(maxDepths)
        if curr_type == 'click' and positions:
            merged_item['position'] = positions[0]
        merged.append(merged_item)
        i = j
    return merged

def compress_page_behavior_logs(pageBehaviorLogs):
    """
    å¯¹ pageBehaviorLogs è¿›è¡Œå‹ç¼©
    """
    # 1. æŒ‰ user åˆå¹¶æ—¥å¿—
    user_logs = defaultdict(list)
    users = {}
    for log in pageBehaviorLogs:
        user = log.get('behaviorData', {}).get('user', {})
        user_name = user.get('userName')
        if user_name:
            user_logs[user_name].append(log)
            users[user_name]=user

    # 2. æŒ‰ windowStart æ’åºï¼Œåˆå¹¶è¿ç»­çš„æ—¶é—´çª—å£çš„æ“ä½œ
    compressed = {}
    for user_name, logs in user_logs.items():
        logs.sort(key=lambda x: parse_iso_time(x.get('windowStart')) or datetime.min)
        merged_windows = {}
        for log in logs:
            if 'windowStart' not in merged_windows:
                merged_windows['windowStart'] = log.get('windowStart')
            merged_windows['windowEnd'] = log.get('windowEnd')
            if 'tabHistory' not in merged_windows:
                merged_windows['tabHistory'] = []

            # åˆå¹¶ tabHistory åˆå¹¶å»é‡
            tab_hist_map = {t['tabId']: t for t in merged_windows['tabHistory']}
            tabHistory = log.get('behaviorData', {}).get('tabHistory', [])
            for t in tabHistory:
                if t['tabId'] not in tab_hist_map:
                    tab_hist_map[t['tabId']] = t
                    del tab_hist_map[t['tabId']]['active']

            # åˆå¹¶ tabBehaviorLogs åˆ°å…·ä½“ tab
            tabBehaviorLogs = log.get('behaviorData', {}).get('tabBehaviorLogs', [])
            for action in tabBehaviorLogs:
                tabid = action["tabId"]
                if tabid not in tab_hist_map:
                    # å¦‚æœ tabId ä¸åœ¨ tabHistory ä¸­ï¼Œè·³è¿‡
                    continue
                else:
                    if 'tabBehaviorLogs' not in tab_hist_map[tabid]:
                        tab_hist_map[tabid]['tabBehaviorLogs'] = []
                    tab_hist_map[tabid]['tabBehaviorLogs'].append(copy.deepcopy(action))

            merged_windows['tabHistory'] = list(tab_hist_map.values())


        # åªä¿ç•™æœ‰æ´»åŠ¨çš„tabï¼Œå°†è¿ç»­çš„æ´»åŠ¨è¿›è¡Œåˆå¹¶
        active_tabHistory = []
        for t in merged_windows['tabHistory']:
            # è¿‡æ»¤æ‰æ²¡æœ‰ tabBehaviorLogs çš„ tabHistory
            if 'tabBehaviorLogs' in t and t['tabBehaviorLogs']:
                # å¯¹æ¯ä¸ª tabHistory çš„ tabBehaviorLogs è¿›è¡Œåˆå¹¶
                t['tabBehaviorLogs'] = merge_tab_behavior_logs(t['tabBehaviorLogs'])
                active_tabHistory.append(t)
        merged_windows['tabHistory'] = active_tabHistory

        compressed[user_name] = {
            'user': users[user_name],
            'windowRanges': {'windowStart':merged_windows['windowStart'],'windowEnd':merged_windows['windowEnd']},
            'tabHistory': merged_windows['tabHistory']
        }
    return compressed

def extract_chunk_data_anomaly(round_index: int, start_time: str, end_time: str, group_id: str, member_list: list) -> Tuple[dict,int]:
    """
    è·å–å½“å‰ chunk å†…æ‰€æœ‰ç”¨æˆ·çš„è¡Œä¸ºæ•°æ®ï¼Œå‡†å¤‡é€å…¥ GPT å¤„ç†ã€‚
    è¿”å›ç»“æ„åŒ…å«æ¯ä½ç”¨æˆ·çš„è¡Œä¸º/æ ‡ç­¾æ•°æ®ã€‚
    """
    total_start_time = time.time()
    logger.info(f"ğŸ” [æ•°æ®é¢„å¤„ç†] å¼€å§‹æå–group_id={group_id}çš„æ•°æ®ï¼Œæ—¶é—´èŒƒå›´ï¼š{start_time} ~ {end_time}")
    
    # ç»Ÿä¸€è§£æä¸ºdatetimeå¯¹è±¡
    start_time_dt = parse_iso_time(start_time)
    end_time_dt = parse_iso_time(end_time)

    user_ids = [m["user_id"] for m in member_list]
    logger.info(f"ğŸ“‹ [æ•°æ®é¢„å¤„ç†] æ´»è·ƒç”¨æˆ·æ•°é‡ï¼š{len(user_ids)}")

    ########################################
    # æŸ¥è¯¢1: speech_transcripts
    query1_start = time.time()
    # æŸ¥è¯¢1.1: startæ—¶é—´åœ¨èŒƒå›´å†…çš„è®°å½•
    speech_transcripts_start = [doc.to_dict() for doc in db.collection("speech_transcripts")
        .where(filter=FieldFilter("group_id", "==", group_id))
        .where(filter=FieldFilter("start", ">=", start_time))
        .where(filter=FieldFilter("start", "<=", end_time))
        .stream()]
    
    # æŸ¥è¯¢1.2: endæ—¶é—´åœ¨èŒƒå›´å†…çš„è®°å½•
    speech_transcripts_end = [doc.to_dict() for doc in db.collection("speech_transcripts")
        .where(filter=FieldFilter("group_id", "==", group_id))
        .where(filter=FieldFilter("end", ">=", start_time))
        .where(filter=FieldFilter("end", "<=", end_time))
        .stream()]
    
    # åˆå¹¶ä¸¤ä¸ªæŸ¥è¯¢ç»“æœå¹¶å»é‡
    speech_transcripts = speech_transcripts_start + speech_transcripts_end
    # ä½¿ç”¨transcript_idå»é‡
    seen_ids = set()
    unique_speech_transcripts = []
    for transcript in speech_transcripts:
        transcript_id = transcript.get("transcript_id")
        if transcript_id not in seen_ids:
            seen_ids.add(transcript_id)
            unique_speech_transcripts.append(transcript)
    
    speech_transcripts = unique_speech_transcripts
    # æŒ‰ user_id ç»Ÿè®¡å‘è¨€æ¬¡æ•°å’Œæ—¶é•¿
    from collections import Counter, defaultdict
    speech_counts = Counter(s["user_id"] for s in speech_transcripts if s.get("user_id"))
    speech_durations = defaultdict(float)
    
    for s in speech_transcripts:
        if s.get("user_id") and s.get("duration"):
            speech_durations[s["user_id"]] += s["duration"]
    
    for m in member_list:
        m["speech_count"] = speech_counts.get(m["user_id"], 0)
        m["speech_duration"] = round(speech_durations.get(m["user_id"], 0), 2)
    query1_duration = time.time() - query1_start
    logger.info(f"ğŸ¤ [æ•°æ®é¢„å¤„ç†] æŸ¥è¯¢1-speech_transcriptså®Œæˆï¼Œè€—æ—¶{query1_duration:.2f}ç§’ï¼Œæ‰¾åˆ°{len(speech_transcripts)}æ¡è®°å½•")

    ########################################
    # æŸ¥è¯¢2: note_edit_history
    query2_start = time.time()
    note_edit_history = []
    if member_list:
        for m in member_list:
            uid = m["user_id"]
            # ç›´æ¥åœ¨æŸ¥è¯¢ä¸­æ·»åŠ æ—¶é—´è¿‡æ»¤æ¡ä»¶
            query = db.collection("note_edit_history")\
                .where(filter=FieldFilter("userId", "==", uid))\
                .where(filter=FieldFilter("updatedAt", ">=", start_time))\
                .where(filter=FieldFilter("updatedAt", "<=", end_time))\
                .stream()
            note_edit_history.extend([doc.to_dict() for doc in query])
    
    query2_duration = time.time() - query2_start
    logger.info(f"ğŸ“ [æ•°æ®é¢„å¤„ç†] æŸ¥è¯¢2-note_edit_historyå®Œæˆï¼Œè€—æ—¶{query2_duration:.2f}ç§’ï¼Œæ‰¾åˆ°{len(note_edit_history)}æ¡è®°å½•")

    ########################################
    # æŸ¥è¯¢3: pageBehaviorLogs
    query3_start = time.time()
    pageBehaviorLogs = []
    
    # æŸ¥è¯¢3.1: group_idåŒ¹é…ä¸”windowStartåœ¨æ—¶é—´èŒƒå›´å†…çš„è®°å½•
    logs_start = []
    query_start = db.collection("pageBehaviorLogs")\
        .where(filter=FieldFilter("behaviorData.user.group_id", "==", group_id))\
        .where(filter=FieldFilter("windowStart", ">=", start_time))\
        .where(filter=FieldFilter("windowStart", "<=", end_time))\
        .stream()
    for doc in query_start:
        logs_start.append(doc.to_dict())
    
    # æŸ¥è¯¢3.2: group_idåŒ¹é…ä¸”windowEndåœ¨æ—¶é—´èŒƒå›´å†…çš„è®°å½•
    logs_end = []
    query_end = db.collection("pageBehaviorLogs")\
        .where(filter=FieldFilter("behaviorData.user.group_id", "==", group_id))\
        .where(filter=FieldFilter("windowEnd", ">=", start_time))\
        .where(filter=FieldFilter("windowEnd", "<=", end_time))\
        .stream()
    for doc in query_end:
        logs_end.append(doc.to_dict())
    
    # åˆå¹¶ä¸¤ä¸ªæŸ¥è¯¢ç»“æœå¹¶å»é‡
    pageBehaviorLogs = logs_start + logs_end
    # ä½¿ç”¨æŸä¸ªå”¯ä¸€æ ‡è¯†å»é‡
    seen_ids = set()
    unique_logs = []
    for log in pageBehaviorLogs:
        log_id = log.get("id") or log.get("_id") or str(log)
        if log_id not in seen_ids:
            seen_ids.add(log_id)
            unique_logs.append(log)
    
    pageBehaviorLogs = unique_logs
    # å‹ç¼© pageBehaviorLogs
    pageBehaviorLogs = compress_page_behavior_logs(pageBehaviorLogs)
    query3_duration = time.time() - query3_start
    logger.info(f"ğŸ–¥ï¸ [æ•°æ®é¢„å¤„ç†] æŸ¥è¯¢3-pageBehaviorLogså®Œæˆï¼Œè€—æ—¶{query3_duration:.2f}ç§’ï¼Œæ‰¾åˆ°{len(unique_logs)}æ¡è®°å½•")

    ########################################
    # æŸ¥è¯¢4: note_contents
    query4_start = time.time()
    note_contents = []
    if member_list:
        uid = member_list[0]["user_id"]
        # ç›´æ¥åœ¨æŸ¥è¯¢ä¸­æ·»åŠ æ—¶é—´è¿‡æ»¤æ¡ä»¶
        query = db.collection("note_contents")\
            .where(filter=FieldFilter("userId", "==", uid))\
            .where(filter=FieldFilter("updatedAt", ">=", start_time))\
            .where(filter=FieldFilter("updatedAt", "<=", end_time))\
            .stream()
        note_contents.extend([doc.to_dict() for doc in query])

    query4_duration = time.time() - query4_start
    logger.info(f"ğŸ“„ [æ•°æ®é¢„å¤„ç†] æŸ¥è¯¢4-note_contentså®Œæˆï¼Œè€—æ—¶{query4_duration:.2f}ç§’ï¼Œæ‰¾åˆ°{len(note_contents)}æ¡è®°å½•")

    chunk_data = {
        "time_range": {
            "start": start_time,
            "end": end_time
        },
        "group_id": group_id,
        "users": member_list,
        "raw_tables": {
            "speech_transcripts": speech_transcripts,
            "note_edit_history": note_edit_history,
            "pageBehaviorLogs": pageBehaviorLogs,
            "unique_note_contents": note_contents
        },
        "speech_counts": speech_counts,
        "speech_durations": dict(speech_durations)
    }

    ########################################
    # æŸ¥è¯¢5: anomaly_analysis_resultså†å²
    query5_start = time.time()
    try:
        # è®¡ç®—æ—¶é—´èŒƒå›´ï¼šä»start_timeå¾€å‰åŠå°æ—¶åˆ°start_time
        history_start_time = start_time_dt - timedelta(minutes=30)
        history_start_time_str = history_start_time.isoformat()
        
        results = db.collection("anomaly_analysis_results") \
            .where(filter=FieldFilter("group_id", "==", group_id)) \
            .where(filter=FieldFilter("created_at", ">=", history_start_time_str)) \
            .where(filter=FieldFilter("created_at", "<=", start_time)) \
            .order_by("created_at", direction="DESCENDING") \
            .limit(2) \
            .stream()
        history = [doc.to_dict() for doc in results]
        anomaly_history = []
        for h in history:
            anomaly_history.append({
                "detail": h.get("detail"),
                "glasses_summary": h.get("glasses_summary"),
                "summary": h.get("summary"),
                "user_data_summary": h.get("user_data_summary"),
                "start_time": h.get("start_time"),
                "end_time": h.get("end_time")
            })
        if len(anomaly_history) == 0:
            chunk_data["anomaly_history"] = None
        else:
            chunk_data["anomaly_history"] = anomaly_history
        query5_duration = time.time() - query5_start
        logger.info(f"ğŸ“š [æ•°æ®é¢„å¤„ç†] æŸ¥è¯¢5-anomaly_analysis_resultså†å²å®Œæˆï¼Œè€—æ—¶{query5_duration:.2f}ç§’ï¼Œæ‰¾åˆ°{len(anomaly_history)}æ¡å†å²è®°å½•")
    except Exception as e:
        query5_duration = time.time() - query5_start
        logger.error(f"âŒ [æ•°æ®é¢„å¤„ç†] æŸ¥è¯¢5-anomaly_analysis_resultså†å²å¤±è´¥ï¼Œè€—æ—¶{query5_duration:.2f}ç§’ï¼š{e}")
        chunk_data["anomaly_history"] = None

    # ä¿å­˜è°ƒè¯•æ–‡ä»¶
    debug_start = time.time()
    from uuid import uuid4
    os.makedirs("debug_anomaly_outputs", exist_ok=True)
    debug_file_path = f"debug_anomaly_outputs/chunk_data_{uuid4().hex}.json"
    with open(debug_file_path, "w", encoding="utf-8") as f:
        json.dump(chunk_data, f, ensure_ascii=False, indent=2)
    debug_duration = time.time() - debug_start
    logger.info(f"ğŸ’¾ [æ•°æ®é¢„å¤„ç†] ä¿å­˜è°ƒè¯•æ–‡ä»¶å®Œæˆï¼Œè€—æ—¶{debug_duration:.2f}ç§’")

    total_duration = time.time() - total_start_time
    logger.info(f"âœ… [æ•°æ®é¢„å¤„ç†] group_id={group_id}æ•°æ®æå–å®Œæˆï¼Œæ€»è€—æ—¶{total_duration:.2f}ç§’")

    # ç”¨æˆ·æ´»åŠ¨è®°å½•çš„å¢é‡æ•°é‡
    increment = len(speech_transcripts) + len(note_edit_history) + len(pageBehaviorLogs) + len(note_contents)

    return chunk_data, increment


def build_cognitive_anomaly_input(chunk_data: dict) -> dict:
    """
    æ„å»ºç”¨äºè®¤çŸ¥åˆ†æä»»åŠ¡çš„ GPT è¾“å…¥ç»“æ„ã€‚
    """
    return {
        "group_id": chunk_data["group_id"],
        "time_range": chunk_data["time_range"],
        "users": chunk_data["users"],
        "speech_transcripts": chunk_data["raw_tables"]["speech_transcripts"],
        "unique_note_contents": chunk_data["raw_tables"]["unique_note_contents"]
    }


def build_behavior_anomaly_input(chunk_data: dict) -> dict:
    """
    æ„å»ºç”¨äºè¡Œä¸ºåˆ†æä»»åŠ¡çš„ GPT è¾“å…¥ç»“æ„ã€‚
    """
    return {
        "group_id": chunk_data["group_id"],
        "time_range": chunk_data["time_range"],
        "users": chunk_data["users"],
        "note_edit_history": chunk_data["raw_tables"]["note_edit_history"],
        "pageBehaviorLogs": chunk_data["raw_tables"]["pageBehaviorLogs"],
        "speech_counts": chunk_data.get("speech_counts", {})
    }


def build_attention_anomaly_input(chunk_data: dict) -> dict:
    """
    æ„å»ºç”¨äºæ³¨æ„åŠ›åˆ†æä»»åŠ¡çš„ GPT è¾“å…¥ç»“æ„ã€‚
    """
    return {
        "group_id": chunk_data["group_id"],
        "time_range": chunk_data["time_range"],
        "users": chunk_data["users"],
        "note_edit_history": chunk_data["raw_tables"]["note_edit_history"],
        "pageBehaviorLogs": chunk_data["raw_tables"]["pageBehaviorLogs"],
        "speech_transcripts": chunk_data["raw_tables"]["speech_transcripts"],
    }

def build_anomaly_history_input(chunk_data: dict) -> dict:
    """
    æ„å»ºç”¨äºå¼‚å¸¸å†å²åˆ†æçš„è¾“å…¥ç»“æ„ã€‚
    """
    return {
        "current_user": chunk_data.get("current_user"),
        "anomaly_history": chunk_data.get("anomaly_history")
    }



if __name__ == '__main__':
    ...

    input_file = "../debug_anomaly_outputs/chunk_data_18b4c9cf636e45e8829738b96f4f53bb.json"
    output_file = "../debug_anomaly_outputs/chunk_data_18b4c9cf636e45e8829738b96f4f53bb_merge.json"
    with open(input_file, 'r', encoding='utf-8') as f:
        pageBehaviorLogs = json.load(f)
    compressed = compress_page_behavior_logs(pageBehaviorLogs["raw_tables"]["pageBehaviorLogs"])
    pageBehaviorLogs["raw_tables"]["pageBehaviorLogs"] = compressed
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(pageBehaviorLogs, f, ensure_ascii=False, indent=2)
    print(f"å‹ç¼©å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° {output_file}")